{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exception code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "CustomException",
     "evalue": "Error occured in python script name [C:\\Users\\vchar\\AppData\\Local\\Temp\\ipykernel_29580\\4159084028.py] line number [17] error message [division by zero]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e, sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Error occured in python script name [C:\\Users\\vchar\\AppData\\Local\\Temp\\ipykernel_29580\\4159084028.py] line number [17] error message [division by zero]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class CustomException(Exception):\n",
    "\n",
    "    def __init__(self, error_message, error_details:sys):\n",
    "        self.error_message = error_message\n",
    "        _, _, exc_tb = error_details.exc_info()\n",
    "        self.lineno = exc_tb.tb_lineno\n",
    "        self.file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Error occured in python script name [{0}] line number [{1}] error message [{2}]\".format(\n",
    "            self.file_name, self.lineno, str(self.error_message))\n",
    "\n",
    "\n",
    "try:\n",
    "    1 / 0\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "LOG_FILE = f\"{dt.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "log_path = os.path.join(os.getcwd(), \"logs\")\n",
    "\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "LOG_FILEPATH = os.path.join(log_path, LOG_FILE)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=LOG_FILEPATH,\n",
    "    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logging.info(\"Log testing executed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# from pathlib import Path\n",
    "\n",
    "# company_bankruptcy_path = Path(r\"C:/\\Users/\\vchar/\\OneDrive/\\Desktop/\\ML Projects/\\portfolio/\\CompanyBankruptcy\")\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# os.chdir(company_bankruptcy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0          1                                           0.370594          \n",
       "1          1                                           0.464291          \n",
       "2          1                                           0.426071          \n",
       "3          1                                           0.399844          \n",
       "4          1                                           0.465022          \n",
       "\n",
       "    ROA(A) before interest and % after tax  \\\n",
       "0                                 0.424389   \n",
       "1                                 0.538214   \n",
       "2                                 0.499019   \n",
       "3                                 0.451265   \n",
       "4                                 0.538432   \n",
       "\n",
       "    ROA(B) before interest and depreciation after tax  \\\n",
       "0                                           0.405750    \n",
       "1                                           0.516730    \n",
       "2                                           0.472295    \n",
       "3                                           0.457733    \n",
       "4                                           0.522298    \n",
       "\n",
       "    Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                 0.601457                      0.601457   \n",
       "1                 0.610235                      0.610235   \n",
       "2                 0.601450                      0.601364   \n",
       "3                 0.583541                      0.583541   \n",
       "4                 0.598783                      0.598783   \n",
       "\n",
       "    Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                0.998969                    0.796887   \n",
       "1                0.998946                    0.797380   \n",
       "2                0.998857                    0.796403   \n",
       "3                0.998700                    0.796967   \n",
       "4                0.998973                    0.797366   \n",
       "\n",
       "    After-tax net Interest Rate   Non-industry income and expenditure/revenue  \\\n",
       "0                      0.808809                                      0.302646   \n",
       "1                      0.809301                                      0.303556   \n",
       "2                      0.808388                                      0.302035   \n",
       "3                      0.808966                                      0.303350   \n",
       "4                      0.809304                                      0.303475   \n",
       "\n",
       "   ...   Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0  ...                     0.716845                    0.009219   \n",
       "1  ...                     0.795297                    0.008323   \n",
       "2  ...                     0.774670                    0.040003   \n",
       "3  ...                     0.739555                    0.003252   \n",
       "4  ...                     0.795016                    0.003878   \n",
       "\n",
       "    No-credit Interval   Gross Profit to Sales  \\\n",
       "0             0.622879                0.601453   \n",
       "1             0.623652                0.610237   \n",
       "2             0.623841                0.601449   \n",
       "3             0.622929                0.583538   \n",
       "4             0.623521                0.598782   \n",
       "\n",
       "    Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                             0.827890              0.290202   \n",
       "1                             0.839969              0.283846   \n",
       "2                             0.836774              0.290189   \n",
       "3                             0.834697              0.281721   \n",
       "4                             0.839973              0.278514   \n",
       "\n",
       "    Degree of Financial Leverage (DFL)  \\\n",
       "0                             0.026601   \n",
       "1                             0.264577   \n",
       "2                             0.026555   \n",
       "3                             0.026697   \n",
       "4                             0.024752   \n",
       "\n",
       "    Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                           0.564050                   1   \n",
       "1                                           0.570175                   1   \n",
       "2                                           0.563706                   1   \n",
       "3                                           0.564663                   1   \n",
       "4                                           0.575617                   1   \n",
       "\n",
       "    Equity to Liability  \n",
       "0              0.016469  \n",
       "1              0.020794  \n",
       "2              0.016474  \n",
       "3              0.023982  \n",
       "4              0.035490  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# company_bankruptcy_path = Path(r\"C:/\\Users/\\vchar/\\OneDrive/\\Desktop/\\ML Projects/\\portfolio/\\CompanyBankruptcy/\\company_bankruptcy\")\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# os.chdir(company_bankruptcy_path)\n",
    "\n",
    "# from company_bankruptcy.exception.exception import CustomException\n",
    "# from company_bankruptcy.logger.logger import logging\n",
    "# from company_bankruptcy.constants.constants import DATABASE_NAME, COLLECTION_NAME, MONGODB_COLLECTION_STR\n",
    "\n",
    "# os.chdir(current_dir)\n",
    "\n",
    "import sys\n",
    "\n",
    "MONGODB_COLLECTION_STR = \"mongodb+srv://vcharchian:12DyeUWoTDa10AJn@cluster0.xbq0vxb.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "DATABASE_NAME = 'bankruptcy'\n",
    "COLLECTION_NAME = 'data'\n",
    "\n",
    "\n",
    "class MongoOps:\n",
    "\n",
    "    def __init__(self, client_url:str, database_name:str=None, collection_name:str=None):\n",
    "        self.client_url = client_url\n",
    "        self.database_name = database_name\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def create_client(self):\n",
    "        logging.info('Initiating MongoClient')\n",
    "        client = pymongo.MongoClient(self.client_url)\n",
    "        logging.info('MongoClient initiated')\n",
    "        return client\n",
    "\n",
    "    def create_database(self):\n",
    "        logging.info('Creating Mongo database')\n",
    "        client = self.create_client()\n",
    "        database = client[self.database_name]\n",
    "        logging.info(f'Mongo database {self.database_name} created')\n",
    "        return database\n",
    "\n",
    "    def create_collection(self):\n",
    "        logging.info('Creating Mongo collection')\n",
    "        database = self.create_database()\n",
    "        collection = database[self.collection_name]\n",
    "        logging.info(f'Mongo collection {self.collection_name} created')\n",
    "        return collection\n",
    "    \n",
    "    def get_database(self, db_name:str):\n",
    "        logging.info(f'Accessing {db_name} database')\n",
    "        client = self.create_client()\n",
    "        database = client[db_name]\n",
    "        logging.info(f'{db_name} database accessed')\n",
    "        return database\n",
    "\n",
    "    def get_collection(self, coll_name:str, db_name:str):\n",
    "        logging.info(f'Accessing {coll_name} collection')\n",
    "        database = self.get_database(db_name)\n",
    "        collection = database[coll_name]\n",
    "        logging.info(f'{coll_name} collection accessed')\n",
    "        return collection\n",
    "\n",
    "    def insert_record(self, record:dict, coll_name:str, db_name:str):\n",
    "        collection = self.get_collection(coll_name, db_name)\n",
    "        logging.info(f'Starting record insertion into {coll_name} collection of {db_name} database')\n",
    "        if isinstance(record, list):\n",
    "            for data in record:\n",
    "                if type(data) != dict:\n",
    "                    logging.info(\"Records' list should have elements as dict\")\n",
    "                    raise TypeError(\"Records' list should have elements as dict\")\n",
    "            collection.insert_many(record)\n",
    "        elif isinstance(record, dict):\n",
    "            collection.insert_one(record)\n",
    "        logging.info(f'Insertion into {coll_name} collection of {db_name} database completed')\n",
    "\n",
    "    def insert_from_file(self, datafile:str, coll_name:str, db_name:str):\n",
    "        logging.info(f'Starting record insertion into {coll_name} collection of {db_name} database from {datafile}')\n",
    "        self.path = datafile\n",
    "\n",
    "        if self.path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.path, encoding='utf-8')\n",
    "        elif self.path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.path, encoding='utf-8')\n",
    "        logging.info('Data is loaded as a pandas dataframe')\n",
    "\n",
    "        logging.info('Converting the data into json')\n",
    "        datajson = json.loads(df.to_json(orient='record'))\n",
    "        logging.info('Conversion to json completed')\n",
    "\n",
    "        collection = self.get_collection(coll_name, db_name)\n",
    "\n",
    "        logging.info('Inserting json data')\n",
    "        collection.insert_many(datajson)\n",
    "        logging.info('Insertion completed')\n",
    "\n",
    "    def get_records(self, coll_name:str, db_name:str):\n",
    "        collection = self.get_collection(coll_name, db_name)\n",
    "        retrieved_data = pd.DataFrame(list(collection.find()))\n",
    "        try:\n",
    "            retrieved_data.drop(columns='_id', inplace=True)\n",
    "            logging.info('Loading the data from the database completed')\n",
    "        except Exception as e:\n",
    "            retrieved_data = pd.DataFrame()\n",
    "            logging.info('Loading the data from the database failed')\n",
    "            raise CustomException(e, sys)\n",
    "        return retrieved_data\n",
    "\n",
    "mongo_instance = MongoOps(\n",
    "    client_url=MONGODB_COLLECTION_STR\n",
    ")\n",
    "\n",
    "retrieved_data = mongo_instance.get_records(coll_name=COLLECTION_NAME, db_name=DATABASE_NAME)\n",
    "retrieved_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    raw_data_path:str = os.path.join('artifacts', 'data.csv')\n",
    "    train_data_path:str = os.path.join('artifacts', 'train_data.csv')\n",
    "    test_data_path:str = os.path.join('artifacts', 'test_data.csv')\n",
    "\n",
    "class DataIngestion:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ingestion_config = DataIngestionConfig()\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info('Data ingestion started')\n",
    "        try:\n",
    "            logging.info('Reading the raw data')\n",
    "            mongo_instance = MongoOps(\n",
    "                client_url=MONGODB_COLLECTION_STR\n",
    "            )\n",
    "            data = mongo_instance.get_records(coll_name=COLLECTION_NAME, db_name=DATABASE_NAME)\n",
    "            logging.info('Data loaded')\n",
    "            os.makedirs(os.path.dirname(os.path.join(self.ingestion_config.raw_data_path)), exist_ok=True)\n",
    "            logging.info('Saving the data')\n",
    "            data.to_csv(self.ingestion_config.raw_data_path, index=False)\n",
    "            logging.info('Data saved')\n",
    "            logging.info('Splitting the data into train and test sets')\n",
    "            train_df, test_df = train_test_split(\n",
    "                data, \n",
    "                test_size=0.1, \n",
    "                random_state=13, \n",
    "                stratify=data['Bankrupt?']\n",
    "            )\n",
    "            logging.info('Saving train and test sets')\n",
    "            train_df.to_csv(self.ingestion_config.train_data_path, index=False)\n",
    "            test_df.to_csv(self.ingestion_config.test_data_path, index=False)\n",
    "            logging.info('Sets are saved')\n",
    "            logging.info('Data ingestion completed')\n",
    "            return (self.ingestion_config.train_data_path, self.ingestion_config.test_data_path)\n",
    "        except Exception as e:\n",
    "            logging.info()\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "data_ingestion_obj = DataIngestion()\n",
    "train_path, test_path = data_ingestion_obj.initiate_data_ingestion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import r_regression, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "import shap\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def save_object(file_path, obj):\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            pickle.dump(obj, file_obj)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise CustomException(e, sys)\n",
    "    \n",
    "def evaluate_model(X_train,y_train,X_test,y_test,models):\n",
    "    try:\n",
    "        report = {}\n",
    "        for i in range(len(models)):\n",
    "            model = list(models.values())[i]\n",
    "            # Train model\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            \n",
    "\n",
    "            # Predict Testing data\n",
    "            y_test_pred =model.predict(X_test)\n",
    "\n",
    "            # Get R2 scores for train and test data\n",
    "            #train_model_score = r2_score(ytrain,y_train_pred)\n",
    "            test_model_score = r2_score(y_test,y_test_pred)\n",
    "\n",
    "            report[list(models.keys())[i]] =  test_model_score\n",
    "\n",
    "        return report\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info('Exception occured during model training')\n",
    "        raise CustomException(e,sys)\n",
    "    \n",
    "def load_object(file_path):\n",
    "    try:\n",
    "        with open(file_path,'rb') as file_obj:\n",
    "            return pickle.load(file_obj)\n",
    "    except Exception as e:\n",
    "        logging.info('Exception Occured in load_object function utils')\n",
    "        raise CustomException(e,sys)\n",
    "    \n",
    "def get_shap_features(shap_values, features, topk=10):\n",
    "    '''\n",
    "    Returns topk features selected using shap values\n",
    "\n",
    "    Args:\n",
    "        shap_values (object): shap explainer\n",
    "        features (list): list of features' name\n",
    "\n",
    "    Returns:\n",
    "        list: topk features derived from shap values\n",
    "    '''\n",
    "    # Calculates the feature importance (mean absolute shap value) for each feature\n",
    "    importances = []\n",
    "    for i in range(shap_values.values.shape[1]):\n",
    "        importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
    "    # Calculates the normalized version\n",
    "    importances_norm = softmax(importances)\n",
    "    # Organize the importances and columns in a dictionary\n",
    "    feature_importances = {fea: imp for imp, fea in zip(importances, features)}\n",
    "    feature_importances_norm = {fea: imp for imp, fea in zip(importances_norm, features)}\n",
    "    # Sorts the dictionary\n",
    "    feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
    "    feature_importances_norm= {k: v for k, v in sorted(feature_importances_norm.items(), key=lambda item: item[1], reverse = True)}\n",
    "    # Prints the feature importances\n",
    "    selected_topk_feats = []\n",
    "    \n",
    "    for idx, (k, v) in enumerate(feature_importances.items()):\n",
    "        # print(f\"{k} -> {v:.4f} (softmax = {feature_importances_norm[k]:.4f})\")\n",
    "        if idx <=topk:\n",
    "            selected_topk_feats.append(k)\n",
    "\n",
    "    return selected_topk_feats\n",
    "\n",
    "class FSelector():\n",
    "    '''\n",
    "    Helps to select features based on BorutaPy, RFE, and various statistics\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, y, num_feats, ordinal_feats, nominal_feats, model, is_target_cat=True, select_n_feats=15):\n",
    "        '''\n",
    "        Initializes some parameters\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contains features' values\n",
    "            y (pd.DataFrame): contains target values\n",
    "            num_feats (list): list of numerical features' names\n",
    "            ordinal_feats (list): list of ordinal features' names\n",
    "            nominal_feats (list): list of nominal features' names\n",
    "            model (model object): can be any type of model like RandomForest, LogisticRegression, etc.\n",
    "            is_target_cat (bool): indicates whether the target is categorical or not\n",
    "            select_n_feats (int): specifies the number of features to output\n",
    "        '''\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_feats = num_feats\n",
    "        self.ordinal_feats = ordinal_feats\n",
    "        self.nominal_feats = nominal_feats\n",
    "        self.model = model\n",
    "        self.is_target_cat = is_target_cat\n",
    "        self.select_n_feats = select_n_feats\n",
    "\n",
    "    def calculate_vif(self, X):\n",
    "    \n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"features\"] = X.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "        return vif\n",
    "\n",
    "    def select_feats_via_vif(self):\n",
    "\n",
    "        num_features = self.num_feats.copy()\n",
    "\n",
    "        vif_df = self.calculate_vif(self.X[num_features])\n",
    "\n",
    "        while vif_df[vif_df['VIF']>=10].shape[0] != 0:\n",
    "            vif_df.sort_values('VIF', ascending=False, inplace=True)\n",
    "            vif_df.reset_index(drop=True, inplace=True)\n",
    "            # print(vif_df)\n",
    "            elimination_candidate = vif_df.iloc[0]['features']\n",
    "            # print(elimination_candidate)\n",
    "            num_features = [i for i in num_features if i!=elimination_candidate]\n",
    "            new_X = self.X[num_features]\n",
    "            vif_df = self.calculate_vif(new_X)\n",
    "\n",
    "        return list(vif_df['features'].values)\n",
    "    \n",
    "    def get_spearmanr(self, X, y):\n",
    "        # return np.array([stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])])\n",
    "        spearman_values = [stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        temp_sp_df = pd.DataFrame({'spearman': spearman_values, 'feats': list(X.columns)})\n",
    "        temp_sp_df['abs_spearman'] = np.abs(temp_sp_df['spearman'])\n",
    "        temp_sp_df.sort_values('abs_spearman', ascending=False, inplace=True)\n",
    "        temp_sp_df.reset_index(drop=True, inplace=True)\n",
    "        return temp_sp_df.iloc[:15]['feats'].to_list()\n",
    "    \n",
    "    def get_kendalltau(self, X, y):\n",
    "        # return [stats.kendalltau(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        kendall_values = [stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        temp_ken_df = pd.DataFrame({'kendall': kendall_values, 'feats': list(X.columns)})\n",
    "        temp_ken_df['abs_kendall'] = np.abs(temp_ken_df['kendall'])\n",
    "        temp_ken_df.sort_values('abs_kendall', ascending=False, inplace=True)\n",
    "        temp_ken_df.reset_index(drop=True, inplace=True)\n",
    "        return temp_ken_df.iloc[:15]['feats'].to_list()\n",
    "    \n",
    "    def get_pointbiserialr(self, X, y):\n",
    "        return [stats.pointbiserialr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "    \n",
    "    def get_boruta_feats(self):\n",
    "        feat_selector = BorutaPy(self.model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        feat_selector.fit(np.array(self.X), np.array(self.y))\n",
    "        boruta_selected_features = list(self.X.iloc[:, feat_selector.support_].columns)\n",
    "        return boruta_selected_features\n",
    "    \n",
    "    def get_kbest(self, X, feats_list, metric):\n",
    "        selector = SelectKBest(metric, k=self.select_n_feats)\n",
    "        selector.fit_transform(X[feats_list], self.y)\n",
    "        selected_feats_idxs_list = list(selector.get_support(indices=True))\n",
    "        column_names = [feats_list[i] for i in selected_feats_idxs_list]\n",
    "        return column_names\n",
    "    \n",
    "    def get_rfe_feats(self):\n",
    "        model_rfe = RFE(self.model, n_features_to_select=self.select_n_feats)\n",
    "        model_rfe.fit(self.X, self.y)\n",
    "        model_rfe_feats = list(self.X.iloc[:, list(model_rfe.support_)].columns)\n",
    "        return model_rfe_feats\n",
    "    \n",
    "    # def get_shap_feats(self, feats_list, topk=10):\n",
    "    #     model = self.model\n",
    "    #     X = self.X[feats_list]\n",
    "    #     model.fit(self.X, self.y)\n",
    "    #     explainer = shap.Explainer(model.predict, X, max_evals = int(2 * X.shape[1] + 1), verbose=0)\n",
    "    #     shap_values = explainer(X)\n",
    "    #     selected_shap_features = get_feature_importances_shap_values(\n",
    "    #         shap_values, features=list(X.columns), topk=topk\n",
    "    #     )\n",
    "    #     return selected_shap_features\n",
    "    \n",
    "    def get_features(self):\n",
    "\n",
    "        if self.num_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.num_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                # self.num_kendalltau_feats = self.get_kendalltau(self.X[self.num_feats], self.y)\n",
    "                self.num_f_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=f_classif)\n",
    "                self.num_mi_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=mutual_info_classif)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "                self.selected_num_feats = []\n",
    "                # self.selected_num_feats.extend(self.num_kendalltau_feats)\n",
    "                self.selected_num_feats.extend(self.num_f_feats)\n",
    "                self.selected_num_feats.extend(self.num_mi_feats)\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.vif_feats = self.select_feats_via_vif()\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.num_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.pearson_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=r_regression, k=self.select_n_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "                # self.num_spearmanr_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=stats.spearmanr, k=self.select_n_feats)\n",
    "                # self.num_kendalltau_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=stats.kendalltau, k=self.select_n_feats)\n",
    "                self.num_spearmanr_feats = self.get_spearmanr(self.X[self.num_feats], self.y)\n",
    "                self.num_kendalltau_feats = self.get_kendalltau(self.X[self.num_feats], self.y)\n",
    "                # self.num_spearmanr_feats = SelectKBest(self.get_spearmanr, k=self.select_n_feats).fit_transform(self.X[self.num_feats], self.y)\n",
    "                # self.num_kendalltau_feats = SelectKBest(self.get_kendalltau, k=self.select_n_feats).fit_transform(self.X[self.num_feats], self.y)\n",
    "\n",
    "                self.selected_num_feats = []\n",
    "                self.selected_num_feats.extend(self.pearson_feats)\n",
    "                self.selected_num_feats.extend(self.num_spearmanr_feats)\n",
    "                self.selected_num_feats.extend(self.num_kendalltau_feats)\n",
    "                # self.selected_num_feats = list(set(self.selected_num_feats))\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.selected_num_feats = []\n",
    "\n",
    "        if self.ordinal_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.ordinal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.ordinal_mi_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=mutual_info_classif)\n",
    "                self.ordinal_chi2_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=chi2)\n",
    "\n",
    "                self.selected_ordinal_feats = []\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_mi_feats)\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_chi2_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.ordinal_spearmanr_feats = self.get_spearmanr(self.X[self.ordinal_feats], self.y)\n",
    "                self.ordinal_kendalltau_feats = self.get_kendalltau(self.X[self.ordinal_feats], self.y)\n",
    "\n",
    "                # self.ordinal_spearmanr_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=stats.spearmanr, k=self.select_n_feats)\n",
    "                # self.ordinal_kendalltau_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=stats.kendalltau, k=self.select_n_feats)\n",
    "\n",
    "                # self.ordinal_spearmanr_feats = SelectKBest(self.get_spearmanr, k=self.select_n_feats).fit_transform(self.X[self.ordinal_feats], self.y)\n",
    "                # self.ordinal_kendalltau_feats = SelectKBest(self.get_kendalltau, k=self.select_n_feats).fit_transform(self.X[self.ordinal_feats], self.y)\n",
    "\n",
    "                self.selected_ordinal_feats = []\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_spearmanr_feats)\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_kendalltau_feats)\n",
    "                # self.selected_ordinal_feats = list(set(self.selected_ordinal_feats))\n",
    "                \n",
    "        else:\n",
    "            self.selected_ordinal_feats = []\n",
    "\n",
    "        if self.nominal_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "                \n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.nominal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.nominal_mi_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=mutual_info_classif)\n",
    "                self.nominal_chi2_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=chi2)\n",
    "\n",
    "                self.selected_nominal_feats = []\n",
    "                self.selected_nominal_feats.extend(self.nominal_mi_feats)\n",
    "                self.selected_nominal_feats.extend(self.nominal_chi2_feats)\n",
    "                \n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.nominal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.f_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=f_classif, k=self.select_n_feats)\n",
    "                self.mi_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=mutual_info_regression, k=self.select_n_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "                # # self.f_feats = f_classif(self.X[self.nominal_feats], self.y)[0]\n",
    "                # self.f_feats = SelectKBest(f_classif, k=self.select_n_feats).fit_transform(self.X[self.nominal_feats], self.y).columns\n",
    "                \n",
    "                # # self.mi_feats = mutual_info_regression(self.X[self.nominal_feats], self.y)\n",
    "                # self.mi_feats = SelectKBest(mutual_info_regression, k=self.select_n_feats).fit_transform(self.X[self.nominal_feats], self.y).columns\n",
    "\n",
    "                self.selected_nominal_feats = []\n",
    "                self.selected_nominal_feats.extend(self.f_feats)\n",
    "                self.selected_nominal_feats.extend(self.mi_feats)\n",
    "                # self.selected_nominal_feats = list(set(self.selected_nominal_feats))\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.selected_nominal_feats = []\n",
    "\n",
    "        if self.model is not None:\n",
    "            # np.int = np.int32\n",
    "            # np.float = np.float64\n",
    "            # np.bool = np.bool_\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.boruta_feats =  self.get_boruta_feats()\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.rfe_feats = self.get_rfe_feats()\n",
    "        else:\n",
    "            self.boruta_feats = []\n",
    "            self.rfe_feats = []\n",
    "\n",
    "            \n",
    "        if len(self.selected_num_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_num_feats.extend(self.boruta_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_num_feats.extend(self.rfe_feats)\n",
    "            num_feats_dict = dict(Counter(self.selected_num_feats))\n",
    "            self.selected_num_feats = [i for i in num_feats_dict if num_feats_dict[i] >= 2]\n",
    "\n",
    "\n",
    "        if len(self.selected_ordinal_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_ordinal_feats.extend(self.boruta_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_ordinal_feats.extend(self.rfe_feats)\n",
    "            ordinal_feats_dict = dict(Counter(self.selected_ordinal_feats))\n",
    "            self.selected_ordinal_feats = [i for i in ordinal_feats_dict if ordinal_feats_dict[i] >= 2]\n",
    "\n",
    "        if len(self.selected_nominal_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_nominal_feats.extend(self.boruta_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_nominal_feats.extend(self.rfe_feats)\n",
    "            nominal_feats_dict = dict(Counter(self.selected_nominal_feats))\n",
    "            self.selected_nominal_feats = [i for i in nominal_feats_dict if nominal_feats_dict[i] >= 2]\n",
    "\n",
    "        self.selected_feats = []\n",
    "        self.selected_feats.extend(self.selected_num_feats)\n",
    "        self.selected_feats.extend(self.selected_ordinal_feats)\n",
    "        self.selected_feats.extend(self.selected_nominal_feats)\n",
    "        if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "            self.selected_feats.extend(self.boruta_feats)\n",
    "        self.selected_feats = list(set(self.selected_feats))\n",
    "\n",
    "        # self.selected_feats = self.get_shap_feats(self.selected_feats)\n",
    "\n",
    "        return self.selected_feats\n",
    "\n",
    "def create_feature_selection_dict(data, cv_fold_list, numerical_features, nominal_features):\n",
    "    '''\n",
    "    Returns feature selection dictionary for 4 different models\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): train data \n",
    "        cv_fold_list (list): contains tuples of indeces of train and validation data for each fold\n",
    "        numerical_features (list): contains the names of numerical features\n",
    "        nominal_features (list): contains the names of nominal features\n",
    "\n",
    "    Returns:\n",
    "        dict: contains selected features, train and validation scores, models and scalers used\n",
    "    '''\n",
    "\n",
    "    selected_features_dict = {}\n",
    "\n",
    "    for idx in tqdm(range(1)):\n",
    "\n",
    "        X_train = data.iloc[cv_fold_list[idx][0]].reset_index(drop=True)\n",
    "        y_train = data.iloc[cv_fold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "        X_valid = data.iloc[cv_fold_list[idx][1]].reset_index(drop=True)\n",
    "        y_valid = data.iloc[cv_fold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "        new_numerical_features = []\n",
    "        for feat in numerical_features:\n",
    "            X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "            X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "            new_numerical_features.append(f\"feat{numerical_features.index(feat)}\")\n",
    "\n",
    "        numerical_features.extend(new_numerical_features)\n",
    "\n",
    "        # getting categorical features\n",
    "        categorical_features = nominal_features.copy()\n",
    "\n",
    "        #getting all features\n",
    "        all_features = []\n",
    "        all_features.extend(categorical_features)\n",
    "        all_features.extend(numerical_features)\n",
    "\n",
    "        X_train = X_train[all_features]\n",
    "        X_valid = X_valid[all_features]\n",
    "\n",
    "        models_list = [RandomForestClassifier(), XGBClassifier(), LogisticRegression(), SVC(probability=True)]\n",
    "        model_names_list = ['RandomForestClassifier', 'XGBClassifier', 'LogisticRegression', 'SVC']\n",
    "\n",
    "        for model_idx in tqdm(range(len(model_names_list))):\n",
    "\n",
    "            model_name = model_names_list[model_idx]\n",
    "\n",
    "            selected_features_dict[model_name] = {}\n",
    "\n",
    "            # feature selection\n",
    "            model = models_list[model_idx]\n",
    "\n",
    "            if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "                X_train2 = scaler.fit_transform(X_train[numerical_features])\n",
    "                X_train2 = pd.DataFrame(X_train2, columns=numerical_features)\n",
    "                X_train2 = pd.concat([X_train2, X_train[categorical_features]], axis=1)\n",
    "\n",
    "                fselector = FSelector(\n",
    "                    X=X_train2, \n",
    "                    y=y_train, \n",
    "                    num_feats=numerical_features, \n",
    "                    ordinal_feats=None, \n",
    "                    nominal_feats=nominal_features, \n",
    "                    model=model\n",
    "                )\n",
    "\n",
    "            else:\n",
    "\n",
    "                fselector = FSelector(\n",
    "                    X=X_train, \n",
    "                    y=y_train, \n",
    "                    num_feats=numerical_features, \n",
    "                    ordinal_feats=None, \n",
    "                    nominal_feats=nominal_features, \n",
    "                    model=model\n",
    "                )\n",
    "\n",
    "            selected_features = fselector.get_features()\n",
    "\n",
    "            if len(selected_features) == 0:\n",
    "                continue\n",
    "            \n",
    "            # selecting features using shap values\n",
    "            if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "\n",
    "                X_valid2 = scaler.transform(X_valid[numerical_features])\n",
    "                X_valid2 = pd.DataFrame(X_valid2, columns=numerical_features)\n",
    "                X_valid2 = pd.concat([X_valid2, X_valid[categorical_features]], axis=1)\n",
    "\n",
    "                X_train_filtered = X_train2[selected_features]\n",
    "                X_valid_filtered = X_valid2[selected_features]\n",
    "\n",
    "            else:\n",
    "\n",
    "                X_train_filtered = X_train[selected_features]\n",
    "                X_valid_filtered = X_valid[selected_features]\n",
    "\n",
    "            # model training using selected features\n",
    "            model.fit(X_train_filtered, y_train)\n",
    "\n",
    "            explainer = shap.Explainer(\n",
    "                model.predict,\n",
    "                X_train_filtered, \n",
    "                max_evals = int(2 * X_train_filtered.shape[1] + 1), \n",
    "                verbose=0\n",
    "            )\n",
    "            shap_values = explainer(X_train_filtered)\n",
    "            selected_shap_features = get_shap_features(\n",
    "                shap_values, \n",
    "                features=list(X_train_filtered.columns), \n",
    "                topk=10\n",
    "            )\n",
    "\n",
    "            # model training using shap features\n",
    "            model = models_list[model_idx]\n",
    "            model.fit(X_train_filtered[selected_shap_features], y_train)\n",
    "\n",
    "            # metric calculation\n",
    "            y_train_pred = model.predict(X_train_filtered[selected_shap_features])\n",
    "            y_train_pred_prob = model.predict_proba(X_train_filtered[selected_shap_features])[:, 1]\n",
    "\n",
    "            y_valid_pred = model.predict(X_valid_filtered[selected_shap_features])\n",
    "            y_valid_pred_prob = model.predict_proba(X_valid_filtered[selected_shap_features])[:, 1]\n",
    "\n",
    "            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            train_f1 = f1_score(y_train, y_train_pred)\n",
    "            train_roc_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "\n",
    "            valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "            valid_f1 = f1_score(y_valid, y_valid_pred)\n",
    "            valid_roc_auc = roc_auc_score(y_valid, y_valid_pred_prob)\n",
    "\n",
    "            selected_features_dict[model_name][idx+1] = {}\n",
    "            selected_features_dict[model_name][idx+1]['selected_feats'] = selected_features\n",
    "            selected_features_dict[model_name][idx+1]['selected_shap_feats'] = selected_shap_features\n",
    "            selected_features_dict[model_name][idx+1]['train_acc'] = train_acc\n",
    "            selected_features_dict[model_name][idx+1]['train_f1'] = train_f1\n",
    "            selected_features_dict[model_name][idx+1]['train_roc_auc'] = train_roc_auc\n",
    "            selected_features_dict[model_name][idx+1]['valid_acc'] = valid_acc\n",
    "            selected_features_dict[model_name][idx+1]['valid_f1'] = valid_f1\n",
    "            selected_features_dict[model_name][idx+1]['valid_roc_auc'] = valid_roc_auc\n",
    "            selected_features_dict[model_name][idx+1]['model'] = model\n",
    "            if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "                selected_features_dict[model_name][idx+1]['scaler'] = scaler\n",
    "\n",
    "            # print(f\"##### {model_name} #####\")\n",
    "            # print(f\"Selected features: {selected_features}\")\n",
    "            # print(\"Train:\")\n",
    "            # print(f\"Accuracy: {train_acc:.5f}, F1: {train_f1:.5f}, ROC-AUC: {train_roc_auc:.5f}\")\n",
    "            # print(\"Validation:\")\n",
    "            # print(f\"Accuracy: {valid_acc:.5f}, F1: {valid_f1:.5f}, ROC-AUC: {valid_roc_auc:.5f}\")\n",
    "\n",
    "            logging.info(\"##### %(model_name)s #####\")\n",
    "            logging.info(f\"Selected features: {selected_features}\")\n",
    "            logging.info('Train:')\n",
    "            logging.info(f\"Accuracy: {train_acc:.5f}, F1: {train_f1:.5f}, ROC-AUC: {train_roc_auc:.5f}\")\n",
    "            logging.info('Validation:')\n",
    "            logging.info(f\"Accuracy: {valid_acc:.5f}, F1: {valid_f1:.5f}, ROC-AUC: {valid_roc_auc:.5f}\")\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid, X_train_filtered, X_valid_filtered, model\n",
    "        gc.collect()\n",
    "\n",
    "    return selected_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [107] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "  0%|          | 0/4 [00:15<?, ?it/s]\n",
      "  0%|          | 0/1 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e, sys)\n\u001b[0;32m     74\u001b[0m data_transformation_obj \u001b[38;5;241m=\u001b[39m DataTransformation()\n\u001b[1;32m---> 75\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_transformation_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_data_transformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_path\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 53\u001b[0m, in \u001b[0;36mDataTransformation.initiate_data_transformation\u001b[1;34m(self, train_path, test_path, n_cv_folds)\u001b[0m\n\u001b[0;32m     50\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew columns created\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting feature selection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m selected_features_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_feature_selection_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv_fold_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskfold_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumerical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumerical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnominal_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnominal_features\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature selection completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving feature selection dictionary as pkl file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 478\u001b[0m, in \u001b[0;36mcreate_feature_selection_dict\u001b[1;34m(data, cv_fold_list, numerical_features, nominal_features)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     fselector \u001b[38;5;241m=\u001b[39m FSelector(\n\u001b[0;32m    470\u001b[0m         X\u001b[38;5;241m=\u001b[39mX_train, \n\u001b[0;32m    471\u001b[0m         y\u001b[38;5;241m=\u001b[39my_train, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[0;32m    476\u001b[0m     )\n\u001b[1;32m--> 478\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mfselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 350\u001b[0m, in \u001b[0;36mFSelector.get_features\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m# np.int = np.int32\u001b[39;00m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;66;03m# np.float = np.float64\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;66;03m# np.bool = np.bool_\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, RandomForestClassifier) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, XGBClassifier):\n\u001b[1;32m--> 350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboruta_feats \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_boruta_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, SVC):\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrfe_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rfe_feats()\n",
      "Cell \u001b[1;32mIn[11], line 189\u001b[0m, in \u001b[0;36mFSelector.get_boruta_feats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_boruta_feats\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    188\u001b[0m     feat_selector \u001b[38;5;241m=\u001b[39m BorutaPy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mfeat_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     boruta_selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[:, feat_selector\u001b[38;5;241m.\u001b[39msupport_]\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m boruta_selected_features\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\boruta\\boruta_py.py:201\u001b[0m, in \u001b[0;36mBorutaPy.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    Fits the Boruta feature selection with the provided estimator.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m        The target values.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\boruta\\boruta_py.py:285\u001b[0m, in \u001b[0;36mBorutaPy._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mset_params(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# add shadow attributes, shuffle them and train estimator, get imps\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m cur_imp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_shadows_get_imps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_reg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# get the threshold of shadow importances we will use for rejection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m imp_sha_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(cur_imp[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperc)\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\boruta\\boruta_py.py:412\u001b[0m, in \u001b[0;36mBorutaPy._add_shadows_get_imps\u001b[1;34m(self, X, y, dec_reg)\u001b[0m\n\u001b[0;32m    410\u001b[0m x_sha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_shuffle, \u001b[38;5;241m0\u001b[39m, x_sha)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# get importance of the merged matrix\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m imp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_imp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# separate importances of real and shadow features\u001b[39;00m\n\u001b[0;32m    414\u001b[0m imp_sha \u001b[38;5;241m=\u001b[39m imp[x_cur_w:]\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\boruta\\boruta_py.py:384\u001b[0m, in \u001b[0;36mBorutaPy._get_imp\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_imp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease check your X and y variable. The provided\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    387\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator cannot be fitted to your data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    feature_selection_dict_file_path = os.path.join('artifacts', 'feature_selection_dict.pkl')\n",
    "\n",
    "class DataTransformation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "\n",
    "    def initiate_data_transformation(self, train_path, test_path, n_cv_folds=10):\n",
    "\n",
    "        try:\n",
    "            logging.info('Loading training data')\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            logging.info('Training data loaded')\n",
    "\n",
    "            logging.info('Loading testing data')\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            logging.info('Testing data loaded')\n",
    "\n",
    "            logging.info('Removing Net Income Flag')\n",
    "            train_df.drop(columns=' Net Income Flag', inplace=True)\n",
    "            test_df.drop(columns=' Net Income Flag', inplace=True)\n",
    "            logging.info('Net Income Flag removed')\n",
    "\n",
    "            logging.info('Specifying nominal and numerical features as list')\n",
    "            nominal_features = [' Liability-Assets Flag']\n",
    "            numerical_features = [col for col in train_df.columns if col not in nominal_features and col!='Bankrupt?']\n",
    "            logging.info('Nominal and numerical features specified')\n",
    "\n",
    "            logging.info(f'Creating {n_cv_folds} CV folds for train data')\n",
    "            skfold = StratifiedKFold(n_splits=n_cv_folds, random_state=42, shuffle=True)\n",
    "            skfold_list = []\n",
    "            for train_idxs, valid_idxs in skfold.split(train_df, y=train_df['Bankrupt?']):\n",
    "                skfold_list.append((train_idxs, valid_idxs))\n",
    "            logging.info('CV folds created')\n",
    "            \n",
    "            logging.info('Creating new columns using categorical and numerical iteractions')\n",
    "            for feat in numerical_features:\n",
    "                train_df[f\"feat{numerical_features.index(feat)}\"] = train_df[feat] * train_df[' Liability-Assets Flag']\n",
    "                test_df[f\"feat{numerical_features.index(feat)}\"] = test_df[feat] * test_df[' Liability-Assets Flag']\n",
    "            logging.info('New columns created')\n",
    "\n",
    "            logging.info('Starting feature selection')\n",
    "            selected_features_dict = create_feature_selection_dict(\n",
    "                data=train_df, \n",
    "                cv_fold_list=skfold_list, \n",
    "                numerical_features=numerical_features, \n",
    "                nominal_features=nominal_features\n",
    "            )\n",
    "            logging.info('Feature selection completed')\n",
    "\n",
    "            logging.info('Saving feature selection dictionary as pkl file')\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.feature_selection_dict_file_path,\n",
    "                obj=selected_features_dict\n",
    "            )\n",
    "            logging.info('Dictionary saved')\n",
    "\n",
    "            return (train_df, test_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Error occured during data transformation')\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "data_transformation_obj = DataTransformation()\n",
    "train_df, test_df = data_transformation_obj.initiate_data_transformation(\n",
    "    train_path=train_path, \n",
    "    test_path=test_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
