{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vchar\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import r_regression, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# from BorutaShap import BorutaShap\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import shap\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from bisect import bisect\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data.csv')\n",
    "\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting features' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = [' Liability-Assets Flag'] # Net Income Flag is removed\n",
    "\n",
    "numerical_features = [col for col in data.columns if col not in nominal_features and col!='Bankrupt?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "skfold_list = []\n",
    "for train_idxs, valid_idxs in skfold.split(data, y=data['Bankrupt?']):\n",
    "    skfold_list.append((train_idxs, valid_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap value importance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances_shap_values(shap_values, features, topk=10):\n",
    "    '''\n",
    "    Prints the feature importances based on SHAP values in an ordered way\n",
    "    shap_values -> The SHAP values calculated from a shap.Explainer object\n",
    "    features -> The name of the features, on the order presented to the explainer\n",
    "    '''\n",
    "    # Calculates the feature importance (mean absolute shap value) for each feature\n",
    "    importances = []\n",
    "    for i in range(shap_values.values.shape[1]):\n",
    "        importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
    "    # Calculates the normalized version\n",
    "    importances_norm = softmax(importances)\n",
    "    # Organize the importances and columns in a dictionary\n",
    "    feature_importances = {fea: imp for imp, fea in zip(importances, features)}\n",
    "    feature_importances_norm = {fea: imp for imp, fea in zip(importances_norm, features)}\n",
    "    # Sorts the dictionary\n",
    "    feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
    "    feature_importances_norm= {k: v for k, v in sorted(feature_importances_norm.items(), key=lambda item: item[1], reverse = True)}\n",
    "    # Prints the feature importances\n",
    "    selected_topk_feats = []\n",
    "    \n",
    "    for idx, (k, v) in enumerate(feature_importances.items()):\n",
    "        # print(f\"{k} -> {v:.4f} (softmax = {feature_importances_norm[k]:.4f})\")\n",
    "        if idx <=topk:\n",
    "            selected_topk_feats.append(k)\n",
    "\n",
    "    return selected_topk_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSelector():\n",
    "\n",
    "    def __init__(self, X, y, num_feats, ordinal_feats, nominal_feats, model, is_target_cat=True, select_n_feats=15):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_feats = num_feats\n",
    "        self.ordinal_feats = ordinal_feats\n",
    "        self.nominal_feats = nominal_feats\n",
    "        self.model = model\n",
    "        self.is_target_cat = is_target_cat\n",
    "        self.select_n_feats = select_n_feats\n",
    "\n",
    "    def calculate_vif(self, X):\n",
    "    \n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"features\"] = X.columns\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "        return vif\n",
    "\n",
    "    def select_feats_via_vif(self):\n",
    "\n",
    "        num_features = self.num_feats.copy()\n",
    "\n",
    "        vif_df = self.calculate_vif(self.X[num_features])\n",
    "\n",
    "        while vif_df[vif_df['VIF']>=10].shape[0] != 0:\n",
    "            vif_df.sort_values('VIF', ascending=False, inplace=True)\n",
    "            vif_df.reset_index(drop=True, inplace=True)\n",
    "            # print(vif_df)\n",
    "            elimination_candidate = vif_df.iloc[0]['features']\n",
    "            # print(elimination_candidate)\n",
    "            num_features = [i for i in num_features if i!=elimination_candidate]\n",
    "            new_X = self.X[num_features]\n",
    "            vif_df = self.calculate_vif(new_X)\n",
    "\n",
    "        return list(vif_df['features'].values)\n",
    "    \n",
    "    def get_spearmanr(self, X, y):\n",
    "        # return np.array([stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])])\n",
    "        spearman_values = [stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        temp_sp_df = pd.DataFrame({'spearman': spearman_values, 'feats': list(X.columns)})\n",
    "        temp_sp_df['abs_spearman'] = np.abs(temp_sp_df['spearman'])\n",
    "        temp_sp_df.sort_values('abs_spearman', ascending=False, inplace=True)\n",
    "        temp_sp_df.reset_index(drop=True, inplace=True)\n",
    "        return temp_sp_df.iloc[:15]['feats'].to_list()\n",
    "    \n",
    "    def get_kendalltau(self, X, y):\n",
    "        # return [stats.kendalltau(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        kendall_values = [stats.spearmanr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "        temp_ken_df = pd.DataFrame({'kendall': kendall_values, 'feats': list(X.columns)})\n",
    "        temp_ken_df['abs_kendall'] = np.abs(temp_ken_df['kendall'])\n",
    "        temp_ken_df.sort_values('abs_kendall', ascending=False, inplace=True)\n",
    "        temp_ken_df.reset_index(drop=True, inplace=True)\n",
    "        return temp_ken_df.iloc[:15]['feats'].to_list()\n",
    "    \n",
    "    def get_pointbiserialr(self, X, y):\n",
    "        return [stats.pointbiserialr(X.values[:, f], y.values).correlation for f in range(X.shape[1])]\n",
    "    \n",
    "    def get_boruto_feats(self):\n",
    "        feat_selector = BorutaPy(self.model, n_estimators='auto', verbose=2, random_state=1)\n",
    "        feat_selector.fit(np.array(self.X), np.array(self.y))\n",
    "        boruta_selected_features = list(self.X.iloc[:, feat_selector.support_].columns)\n",
    "        return boruta_selected_features\n",
    "    \n",
    "    def get_kbest(self, X, feats_list, metric):\n",
    "        selector = SelectKBest(metric, k=self.select_n_feats)\n",
    "        selector.fit_transform(X[feats_list], self.y)\n",
    "        selected_feats_idxs_list = list(selector.get_support(indices=True))\n",
    "        column_names = [feats_list[i] for i in selected_feats_idxs_list]\n",
    "        return column_names\n",
    "    \n",
    "    def get_rfe_feats(self):\n",
    "        model_rfe = RFE(self.model, n_features_to_select=self.select_n_feats)\n",
    "        model_rfe.fit(self.X, self.y)\n",
    "        model_rfe_feats = list(self.X.iloc[:, list(model_rfe.support_)].columns)\n",
    "        return model_rfe_feats\n",
    "    \n",
    "    def get_shap_feats(self, feats_list, topk=10):\n",
    "        model = self.model\n",
    "        X = self.X[feats_list]\n",
    "        model.fit(self.X, self.y)\n",
    "        explainer = shap.Explainer(model.predict, X, max_evals = int(2 * X.shape[1] + 1), verbose=0)\n",
    "        shap_values = explainer(X)\n",
    "        selected_shap_features = get_feature_importances_shap_values(\n",
    "            shap_values, features=list(X.columns), topk=topk\n",
    "        )\n",
    "        return selected_shap_features\n",
    "    \n",
    "    def get_votes(self):\n",
    "\n",
    "        if self.num_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.num_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                # self.num_kendalltau_feats = self.get_kendalltau(self.X[self.num_feats], self.y)\n",
    "                self.num_f_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=f_classif)\n",
    "                self.num_mi_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=mutual_info_classif)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "                self.selected_num_feats = []\n",
    "                # self.selected_num_feats.extend(self.num_kendalltau_feats)\n",
    "                self.selected_num_feats.extend(self.num_f_feats)\n",
    "                self.selected_num_feats.extend(self.num_mi_feats)\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.vif_feats = self.select_feats_via_vif()\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.num_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.pearson_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=r_regression, k=self.select_n_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "                # self.num_spearmanr_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=stats.spearmanr, k=self.select_n_feats)\n",
    "                # self.num_kendalltau_feats = self.get_kbest(X=self.X, feats_list=self.num_feats, metric=stats.kendalltau, k=self.select_n_feats)\n",
    "                self.num_spearmanr_feats = self.get_spearmanr(self.X[self.num_feats], self.y)\n",
    "                self.num_kendalltau_feats = self.get_kendalltau(self.X[self.num_feats], self.y)\n",
    "                # self.num_spearmanr_feats = SelectKBest(self.get_spearmanr, k=self.select_n_feats).fit_transform(self.X[self.num_feats], self.y)\n",
    "                # self.num_kendalltau_feats = SelectKBest(self.get_kendalltau, k=self.select_n_feats).fit_transform(self.X[self.num_feats], self.y)\n",
    "\n",
    "                self.selected_num_feats = []\n",
    "                self.selected_num_feats.extend(self.pearson_feats)\n",
    "                self.selected_num_feats.extend(self.num_spearmanr_feats)\n",
    "                self.selected_num_feats.extend(self.num_kendalltau_feats)\n",
    "                # self.selected_num_feats = list(set(self.selected_num_feats))\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.selected_num_feats = []\n",
    "\n",
    "        if self.ordinal_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.ordinal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.ordinal_mi_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=mutual_info_classif)\n",
    "                self.ordinal_chi2_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=chi2)\n",
    "\n",
    "                self.selected_ordinal_feats = []\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_mi_feats)\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_chi2_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.ordinal_spearmanr_feats = self.get_spearmanr(self.X[self.ordinal_feats], self.y)\n",
    "                self.ordinal_kendalltau_feats = self.get_kendalltau(self.X[self.ordinal_feats], self.y)\n",
    "\n",
    "                # self.ordinal_spearmanr_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=stats.spearmanr, k=self.select_n_feats)\n",
    "                # self.ordinal_kendalltau_feats = self.get_kbest(X=self.X, feats_list=self.ordinal_feats, metric=stats.kendalltau, k=self.select_n_feats)\n",
    "\n",
    "                # self.ordinal_spearmanr_feats = SelectKBest(self.get_spearmanr, k=self.select_n_feats).fit_transform(self.X[self.ordinal_feats], self.y)\n",
    "                # self.ordinal_kendalltau_feats = SelectKBest(self.get_kendalltau, k=self.select_n_feats).fit_transform(self.X[self.ordinal_feats], self.y)\n",
    "\n",
    "                self.selected_ordinal_feats = []\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_spearmanr_feats)\n",
    "                self.selected_ordinal_feats.extend(self.ordinal_kendalltau_feats)\n",
    "                # self.selected_ordinal_feats = list(set(self.selected_ordinal_feats))\n",
    "                \n",
    "        else:\n",
    "            self.selected_ordinal_feats = []\n",
    "\n",
    "        if self.nominal_feats is not None:\n",
    "\n",
    "            if self.is_target_cat:\n",
    "                \n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.nominal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.nominal_mi_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=mutual_info_classif)\n",
    "                self.nominal_chi2_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=chi2)\n",
    "\n",
    "                self.selected_nominal_feats = []\n",
    "                self.selected_nominal_feats.extend(self.nominal_mi_feats)\n",
    "                self.selected_nominal_feats.extend(self.nominal_chi2_feats)\n",
    "                \n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp_n_feats =  self.select_n_feats\n",
    "                if len(self.nominal_feats) < self.select_n_feats:\n",
    "                    self.select_n_feats = 'all'\n",
    "\n",
    "                self.f_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=f_classif, k=self.select_n_feats)\n",
    "                self.mi_feats = self.get_kbest(X=self.X, feats_list=self.nominal_feats, metric=mutual_info_regression, k=self.select_n_feats)\n",
    "\n",
    "                self.select_n_feats = temp_n_feats\n",
    "\n",
    "                # # self.f_feats = f_classif(self.X[self.nominal_feats], self.y)[0]\n",
    "                # self.f_feats = SelectKBest(f_classif, k=self.select_n_feats).fit_transform(self.X[self.nominal_feats], self.y).columns\n",
    "                \n",
    "                # # self.mi_feats = mutual_info_regression(self.X[self.nominal_feats], self.y)\n",
    "                # self.mi_feats = SelectKBest(mutual_info_regression, k=self.select_n_feats).fit_transform(self.X[self.nominal_feats], self.y).columns\n",
    "\n",
    "                self.selected_nominal_feats = []\n",
    "                self.selected_nominal_feats.extend(self.f_feats)\n",
    "                self.selected_nominal_feats.extend(self.mi_feats)\n",
    "                # self.selected_nominal_feats = list(set(self.selected_nominal_feats))\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.selected_nominal_feats = []\n",
    "\n",
    "        if self.model is not None:\n",
    "            # np.int = np.int32\n",
    "            # np.float = np.float64\n",
    "            # np.bool = np.bool_\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.boruto_feats =  self.get_boruto_feats()\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.rfe_feats = self.get_rfe_feats()\n",
    "        else:\n",
    "            self.boruto_feats = []\n",
    "            self.rfe_feats = []\n",
    "\n",
    "            \n",
    "        if len(self.selected_num_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_num_feats.extend(self.boruto_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_num_feats.extend(self.rfe_feats)\n",
    "            num_feats_dict = dict(Counter(self.selected_num_feats))\n",
    "            self.selected_num_feats = [i for i in num_feats_dict if num_feats_dict[i] >= 2]\n",
    "\n",
    "\n",
    "        if len(self.selected_ordinal_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_ordinal_feats.extend(self.boruto_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_ordinal_feats.extend(self.rfe_feats)\n",
    "            ordinal_feats_dict = dict(Counter(self.selected_ordinal_feats))\n",
    "            self.selected_ordinal_feats = [i for i in ordinal_feats_dict if ordinal_feats_dict[i] >= 2]\n",
    "\n",
    "        if len(self.selected_nominal_feats) != 0:\n",
    "            if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "                self.selected_nominal_feats.extend(self.boruto_feats)\n",
    "            if not isinstance(self.model, SVC):\n",
    "                self.selected_nominal_feats.extend(self.rfe_feats)\n",
    "            nominal_feats_dict = dict(Counter(self.selected_nominal_feats))\n",
    "            self.selected_nominal_feats = [i for i in nominal_feats_dict if nominal_feats_dict[i] >= 2]\n",
    "\n",
    "        self.selected_feats = []\n",
    "        self.selected_feats.extend(self.selected_num_feats)\n",
    "        self.selected_feats.extend(self.selected_ordinal_feats)\n",
    "        self.selected_feats.extend(self.selected_nominal_feats)\n",
    "        if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, XGBClassifier):\n",
    "            self.selected_feats.extend(self.boruto_feats)\n",
    "        self.selected_feats = list(set(self.selected_feats))\n",
    "\n",
    "        # self.selected_feats = self.get_shap_feats(self.selected_feats)\n",
    "\n",
    "        return self.selected_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t18\n",
      "Rejected: \t141\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t17\n",
      "Rejected: \t142\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t17\n",
      "Rejected: \t142\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t13\n",
      "Rejected: \t146\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t9\n",
      "Rejected: \t150\n",
      "Iteration: \t29 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t7\n",
      "Rejected: \t152\n",
      "Iteration: \t30 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t7\n",
      "Rejected: \t152\n",
      "Iteration: \t31 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t7\n",
      "Rejected: \t152\n",
      "Iteration: \t32 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t6\n",
      "Rejected: \t153\n",
      "Iteration: \t33 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t6\n",
      "Rejected: \t153\n",
      "Iteration: \t34 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t6\n",
      "Rejected: \t153\n",
      "Iteration: \t35 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t36 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t37 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t38 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t39 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t40 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t41 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t42 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t5\n",
      "Rejected: \t154\n",
      "Iteration: \t43 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t4\n",
      "Rejected: \t155\n",
      "Iteration: \t44 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t4\n",
      "Rejected: \t155\n",
      "Iteration: \t45 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t4\n",
      "Rejected: \t155\n",
      "Iteration: \t46 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t47 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t48 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t49 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t51 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t52 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t53 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t54 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t55 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t56 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t57 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t58 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t59 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t60 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t61 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t62 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t63 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t64 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t65 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t66 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t67 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t68 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t69 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t70 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t71 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t72 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t73 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t74 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t75 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t76 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t77 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t78 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t79 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t80 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t81 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t82 / 100\n",
      "Confirmed: \t30\n",
      "Tentative: \t3\n",
      "Rejected: \t156\n",
      "Iteration: \t83 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t84 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t85 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t86 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t87 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t88 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t89 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t90 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t91 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t92 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t93 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t94 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t95 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t96 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t97 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t98 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "Iteration: \t99 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t31\n",
      "Tentative: \t2\n",
      "Rejected: \t156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### RandomForestClassifier #####\n",
      "Selected features: [' Liability-Assets Flag', ' Working Capital to Total Assets', ' Cash/Current Liability', ' Interest Expense Ratio', ' ROA(A) before interest and % after tax', \" Net Income to Stockholder's Equity\", ' After-tax net Interest Rate', ' Net Income to Total Assets', ' Persistent EPS in the Last Four Seasons', ' Total debt/Total net worth', ' Non-industry income and expenditure/revenue', ' Equity to Liability', ' Borrowing dependency', ' ROA(C) before interest and depreciation before interest', ' Per Share Net profit before tax (Yuan ¥)', ' Working Capital/Equity', ' ROA(B) before interest and depreciation after tax', ' Net Value Growth Rate', ' Debt ratio %', ' Interest Coverage Ratio (Interest expense to EBIT)', ' Net worth/Assets', ' Quick Ratio', ' Continuous interest rate (after tax)', ' Net profit before tax/Paid-in capital', ' Net Value Per Share (C)', ' Degree of Financial Leverage (DFL)', ' Net Value Per Share (B)', ' Interest-bearing debt interest rate', ' Liability to Equity', ' Net Value Per Share (A)', ' Inventory/Working Capital', ' Cash/Total Assets']\n",
      "Train:\n",
      "Accuracy: 1.00000, F1: 1.00000, ROC-AUC: 1.00000\n",
      "Validation:\n",
      "Accuracy: 0.97068, F1: 0.40000, ROC-AUC: 0.88830\n",
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t189\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t31\n",
      "Rejected: \t158\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t2\n",
      "Tentative: \t29\n",
      "Rejected: \t158\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t2\n",
      "Tentative: \t29\n",
      "Rejected: \t158\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t2\n",
      "Tentative: \t29\n",
      "Rejected: \t158\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t4\n",
      "Tentative: \t24\n",
      "Rejected: \t161\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t4\n",
      "Tentative: \t24\n",
      "Rejected: \t161\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t4\n",
      "Tentative: \t24\n",
      "Rejected: \t161\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t4\n",
      "Tentative: \t24\n",
      "Rejected: \t161\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t5\n",
      "Tentative: \t20\n",
      "Rejected: \t164\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t5\n",
      "Tentative: \t20\n",
      "Rejected: \t164\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t5\n",
      "Tentative: \t20\n",
      "Rejected: \t164\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t15\n",
      "Rejected: \t167\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t15\n",
      "Rejected: \t167\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t15\n",
      "Rejected: \t167\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t13\n",
      "Rejected: \t168\n",
      "Iteration: \t29 / 100\n",
      "Confirmed: \t9\n",
      "Tentative: \t12\n",
      "Rejected: \t168\n",
      "Iteration: \t30 / 100\n",
      "Confirmed: \t9\n",
      "Tentative: \t12\n",
      "Rejected: \t168\n",
      "Iteration: \t31 / 100\n",
      "Confirmed: \t9\n",
      "Tentative: \t12\n",
      "Rejected: \t168\n",
      "Iteration: \t32 / 100\n",
      "Confirmed: \t11\n",
      "Tentative: \t9\n",
      "Rejected: \t169\n",
      "Iteration: \t33 / 100\n",
      "Confirmed: \t11\n",
      "Tentative: \t9\n",
      "Rejected: \t169\n",
      "Iteration: \t34 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t8\n",
      "Rejected: \t169\n",
      "Iteration: \t35 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t8\n",
      "Rejected: \t169\n",
      "Iteration: \t36 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t8\n",
      "Rejected: \t169\n",
      "Iteration: \t37 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t38 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t39 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t40 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t41 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t42 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t43 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t44 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t45 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t46 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t47 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t48 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t49 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t12\n",
      "Tentative: \t7\n",
      "Rejected: \t170\n",
      "Iteration: \t51 / 100\n",
      "Confirmed: \t13\n",
      "Tentative: \t6\n",
      "Rejected: \t170\n",
      "Iteration: \t52 / 100\n",
      "Confirmed: \t13\n",
      "Tentative: \t6\n",
      "Rejected: \t170\n",
      "Iteration: \t53 / 100\n",
      "Confirmed: \t13\n",
      "Tentative: \t6\n",
      "Rejected: \t170\n",
      "Iteration: \t54 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t55 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t56 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t57 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t58 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t59 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t60 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t61 / 100\n",
      "Confirmed: \t14\n",
      "Tentative: \t5\n",
      "Rejected: \t170\n",
      "Iteration: \t62 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t63 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t64 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t65 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t66 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t67 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t68 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t69 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t70 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t71 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t72 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t73 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t74 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t75 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t76 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t77 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t78 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t79 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t80 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t81 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t82 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t83 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t84 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t85 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t86 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t87 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t88 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t89 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t90 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t91 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t92 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t93 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t94 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t95 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t96 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t97 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t98 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "Iteration: \t99 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t16\n",
      "Tentative: \t2\n",
      "Rejected: \t171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### XGBClassifier #####\n",
      "Selected features: [' Liability-Assets Flag', ' ROA(A) before interest and % after tax', \" Net Income to Stockholder's Equity\", ' Net Income to Total Assets', ' Persistent EPS in the Last Four Seasons', ' Total debt/Total net worth', ' Non-industry income and expenditure/revenue', ' Borrowing dependency', ' ROA(C) before interest and depreciation before interest', ' Per Share Net profit before tax (Yuan ¥)', ' ROA(B) before interest and depreciation after tax', ' Net Value Growth Rate', ' Debt ratio %', ' Pre-tax net Interest Rate', ' Interest Coverage Ratio (Interest expense to EBIT)', ' Net worth/Assets', ' Continuous interest rate (after tax)', ' Net profit before tax/Paid-in capital', ' Net Value Per Share (B)', ' Liability to Equity', ' Inventory/Working Capital']\n",
      "Train:\n",
      "Accuracy: 1.00000, F1: 1.00000, ROC-AUC: 1.00000\n",
      "Validation:\n",
      "Accuracy: 0.97231, F1: 0.51429, ROC-AUC: 0.91557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LogisticRegression #####\n",
      "Selected features: [' Net profit before tax/Paid-in capital', ' ROA(B) before interest and depreciation after tax', ' Persistent EPS in the Last Four Seasons', ' Net Income to Total Assets', ' Liability-Assets Flag', ' Liability to Equity', ' ROA(A) before interest and % after tax', ' Debt ratio %', ' Net worth/Assets', \" Net Income to Stockholder's Equity\", ' Borrowing dependency', ' Per Share Net profit before tax (Yuan ¥)']\n",
      "Train:\n",
      "Accuracy: 0.96922, F1: 0.15842, ROC-AUC: 0.92356\n",
      "Validation:\n",
      "Accuracy: 0.38599, F1: 0.09592, ROC-AUC: 0.92997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [28:33<00:00, 428.33s/it]\n",
      "100%|██████████| 1/1 [28:33<00:00, 1713.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### SVC #####\n",
      "Selected features: [' Net profit before tax/Paid-in capital', ' Persistent EPS in the Last Four Seasons', ' Liability-Assets Flag', ' Liability to Equity', ' ROA(A) before interest and % after tax', ' Debt ratio %', ' Net worth/Assets', \" Net Income to Stockholder's Equity\", ' Borrowing dependency', ' Per Share Net profit before tax (Yuan ¥)']\n",
      "Train:\n",
      "Accuracy: 0.96868, F1: 0.06486, ROC-AUC: 0.80992\n",
      "Validation:\n",
      "Accuracy: 0.96743, F1: 0.00000, ROC-AUC: 0.89588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features_dict = {}\n",
    "\n",
    "for idx in tqdm(range(1)):\n",
    "\n",
    "    X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    new_numerical_features = []\n",
    "    for feat in numerical_features:\n",
    "        X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "        X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "        new_numerical_features.append(f\"feat{numerical_features.index(feat)}\")\n",
    "\n",
    "    numerical_features.extend(new_numerical_features)\n",
    "\n",
    "    # getting categorical features\n",
    "    categorical_features = nominal_features.copy()\n",
    "\n",
    "    #getting all features\n",
    "    all_features = []\n",
    "    all_features.extend(categorical_features)\n",
    "    all_features.extend(numerical_features)\n",
    "\n",
    "    X_train = X_train[all_features]\n",
    "    X_valid = X_valid[all_features]\n",
    "\n",
    "    models_list = [RandomForestClassifier(), XGBClassifier(), LogisticRegression(), SVC(probability=True)]\n",
    "    model_names_list = ['RandomForestClassifier', 'XGBClassifier', 'LogisticRegression', 'SVC']\n",
    "\n",
    "    for model_idx in tqdm(range(len(model_names_list))):\n",
    "\n",
    "        model_name = model_names_list[model_idx]\n",
    "\n",
    "        selected_features_dict[model_name] = {}\n",
    "\n",
    "        # feature selection\n",
    "        model = models_list[model_idx]\n",
    "\n",
    "        if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            X_train2 = scaler.fit_transform(X_train[numerical_features])\n",
    "            X_train2 = pd.DataFrame(X_train2, columns=numerical_features)\n",
    "            X_train2 = pd.concat([X_train2, X_train[categorical_features]], axis=1)\n",
    "\n",
    "            fselector = FSelector(\n",
    "                X=X_train2, \n",
    "                y=y_train, \n",
    "                num_feats=numerical_features, \n",
    "                ordinal_feats=None, \n",
    "                nominal_feats=nominal_features, \n",
    "                model=model\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            fselector = FSelector(\n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                num_feats=numerical_features, \n",
    "                ordinal_feats=None, \n",
    "                nominal_feats=nominal_features, \n",
    "                model=model\n",
    "            )\n",
    "\n",
    "        selected_features = fselector.get_votes()\n",
    "\n",
    "        if len(selected_features) == 0:\n",
    "            continue\n",
    "\n",
    "        # model training\n",
    "        model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "        # metric calculation\n",
    "        y_train_pred = model.predict(X_train[selected_features])\n",
    "        y_train_pred_prob = model.predict_proba(X_train[selected_features])[:, 1]\n",
    "\n",
    "        if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "            X_valid2 = scaler.transform(X_valid[numerical_features])\n",
    "            X_valid2 = pd.DataFrame(X_valid2, columns=numerical_features)\n",
    "            X_valid2 = pd.concat([X_valid2, X_valid[categorical_features]], axis=1)\n",
    "            y_valid_pred = model.predict(X_valid2[selected_features])\n",
    "        else:\n",
    "            y_valid_pred = model.predict(X_valid[selected_features])\n",
    "        y_valid_pred_prob = model.predict_proba(X_valid[selected_features])[:, 1]\n",
    "\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "\n",
    "        valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        valid_f1 = f1_score(y_valid, y_valid_pred)\n",
    "        valid_roc_auc = roc_auc_score(y_valid, y_valid_pred_prob)\n",
    "\n",
    "        selected_features_dict[model_name][idx+1] = {}\n",
    "        selected_features_dict[model_name][idx+1]['selected_feats'] = selected_features\n",
    "        selected_features_dict[model_name][idx+1]['train_acc'] = train_acc\n",
    "        selected_features_dict[model_name][idx+1]['train_f1'] = train_f1\n",
    "        selected_features_dict[model_name][idx+1]['train_roc_auc'] = train_roc_auc\n",
    "        selected_features_dict[model_name][idx+1]['valid_acc'] = valid_acc\n",
    "        selected_features_dict[model_name][idx+1]['valid_f1'] = valid_f1\n",
    "        selected_features_dict[model_name][idx+1]['valid_roc_auc'] = valid_roc_auc\n",
    "        selected_features_dict[model_name][idx+1]['model'] = model\n",
    "        if isinstance(model, LogisticRegression) or isinstance(model, SVC):\n",
    "            selected_features_dict[model_name][idx+1]['scaler'] = scaler\n",
    "\n",
    "        print(f\"##### {model_name} #####\")\n",
    "        print(f\"Selected features: {selected_features}\")\n",
    "        print(\"Train:\")\n",
    "        print(f\"Accuracy: {train_acc:.5f}, F1: {train_f1:.5f}, ROC-AUC: {train_roc_auc:.5f}\")\n",
    "        print(\"Validation:\")\n",
    "        print(f\"Accuracy: {valid_acc:.5f}, F1: {valid_f1:.5f}, ROC-AUC: {valid_roc_auc:.5f}\")\n",
    "\n",
    "    del X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Liability-Assets Flag',\n",
       " ' Working Capital to Total Assets',\n",
       " ' Cash/Current Liability',\n",
       " ' Interest Expense Ratio',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' After-tax net Interest Rate',\n",
       " ' Net Income to Total Assets',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Total debt/Total net worth',\n",
       " ' Non-industry income and expenditure/revenue',\n",
       " ' Equity to Liability',\n",
       " ' Borrowing dependency',\n",
       " ' ROA(C) before interest and depreciation before interest',\n",
       " ' Per Share Net profit before tax (Yuan ¥)',\n",
       " ' Working Capital/Equity',\n",
       " ' ROA(B) before interest and depreciation after tax',\n",
       " ' Net Value Growth Rate',\n",
       " ' Debt ratio %',\n",
       " ' Interest Coverage Ratio (Interest expense to EBIT)',\n",
       " ' Net worth/Assets',\n",
       " ' Quick Ratio',\n",
       " ' Continuous interest rate (after tax)',\n",
       " ' Net profit before tax/Paid-in capital',\n",
       " ' Net Value Per Share (C)',\n",
       " ' Degree of Financial Leverage (DFL)',\n",
       " ' Net Value Per Share (B)',\n",
       " ' Interest-bearing debt interest rate',\n",
       " ' Liability to Equity',\n",
       " ' Net Value Per Share (A)',\n",
       " ' Inventory/Working Capital',\n",
       " ' Cash/Total Assets']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_dict['RandomForestClassifier'][1]['selected_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Liability-Assets Flag',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' Net Income to Total Assets',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Total debt/Total net worth',\n",
       " ' Non-industry income and expenditure/revenue',\n",
       " ' Borrowing dependency',\n",
       " ' ROA(C) before interest and depreciation before interest',\n",
       " ' Per Share Net profit before tax (Yuan ¥)',\n",
       " ' ROA(B) before interest and depreciation after tax',\n",
       " ' Net Value Growth Rate',\n",
       " ' Debt ratio %',\n",
       " ' Pre-tax net Interest Rate',\n",
       " ' Interest Coverage Ratio (Interest expense to EBIT)',\n",
       " ' Net worth/Assets',\n",
       " ' Continuous interest rate (after tax)',\n",
       " ' Net profit before tax/Paid-in capital',\n",
       " ' Net Value Per Share (B)',\n",
       " ' Liability to Equity',\n",
       " ' Inventory/Working Capital']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_dict['XGBClassifier'][1]['selected_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Net profit before tax/Paid-in capital',\n",
       " ' ROA(B) before interest and depreciation after tax',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Net Income to Total Assets',\n",
       " ' Liability-Assets Flag',\n",
       " ' Liability to Equity',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " ' Debt ratio %',\n",
       " ' Net worth/Assets',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' Borrowing dependency',\n",
       " ' Per Share Net profit before tax (Yuan ¥)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_dict['LogisticRegression'][1]['selected_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Net profit before tax/Paid-in capital',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Liability-Assets Flag',\n",
       " ' Liability to Equity',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " ' Debt ratio %',\n",
       " ' Net worth/Assets',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' Borrowing dependency',\n",
       " ' Per Share Net profit before tax (Yuan ¥)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_dict['SVC'][1]['selected_feats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 5524it [04:31, 20.04it/s]                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Borrowing dependency',\n",
       " ' Degree of Financial Leverage (DFL)',\n",
       " ' Interest Expense Ratio',\n",
       " ' Interest Coverage Ratio (Interest expense to EBIT)',\n",
       " ' Cash/Total Assets',\n",
       " ' Quick Ratio',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Working Capital to Total Assets',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' Working Capital/Equity',\n",
       " ' Equity to Liability']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feat in numerical_features:\n",
    "    data[f\"feat{numerical_features.index(feat)}\"] = data[feat] * data[' Liability-Assets Flag']\n",
    "\n",
    "idx = 0\n",
    "\n",
    "X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "for feat in numerical_features:\n",
    "    X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "    X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "selected_features = selected_features_dict['RandomForestClassifier'][1]['selected_feats']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_valid = X_valid[selected_features]\n",
    "\n",
    "model = selected_features_dict['RandomForestClassifier'][1]['model']\n",
    "explainer = shap.Explainer(model.predict, X_train, max_evals = int(2 * X_train.shape[1] + 1), verbose=0)\n",
    "shap_values = explainer(X_train)\n",
    "selected_shap_features = get_feature_importances_shap_values(\n",
    "    shap_values, features=list(X_train.columns), topk=10\n",
    ")\n",
    "selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats'] = selected_shap_features\n",
    "selected_shap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
      "{'max_depth': 8, 'n_estimators': 500}\n",
      "0.9360679679251616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9544765840220386"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators_list = [5, 10, 15, 25, 50, 100, 120, 300, 500]#, 800, 1200]\n",
    "\n",
    "max_depth_list = [2, 3, 5, 8, 15, 25, 30, None]\n",
    "\n",
    "# min_samples_split_list = [2, 5, 10, 15, 100]\n",
    "\n",
    "# min_samples_leaf_list = [2, 5, 10]\n",
    "\n",
    "# max_features_list = ['log2', 'sqrt', None]\n",
    "\n",
    "params_dict={\n",
    "    'n_estimators': n_estimators_list,\n",
    "    'max_depth': max_depth_list,\n",
    "    # 'min_samples_split': min_samples_split_list,\n",
    "    # 'min_samples_leaf': min_samples_leaf_list,\n",
    "    # 'max_features': max_features_list\n",
    "}\n",
    "\n",
    "rf_gscv = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid=params_dict,\n",
    "    scoring='roc_auc',\n",
    "    cv=skfold_list,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "\n",
    "# model training\n",
    "rf_gscv.fit(data[selected_shap_features], data['Bankrupt?'])\n",
    "\n",
    "print(rf_gscv.best_params_)\n",
    "print(rf_gscv.best_score_)\n",
    "\n",
    "for feat in numerical_features:\n",
    "    test_data[f\"feat{numerical_features.index(feat)}\"] = test_data[feat] * test_data[' Liability-Assets Flag']\n",
    "\n",
    "X_test = test_data[selected_shap_features]\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "X_train = data[selected_shap_features]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "rf_y_train_pred_prob = rf_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = rf_gscv.predict(X_test[selected_features])\n",
    "rf_y_test_pred_prob = rf_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pickle.dump(rf_gscv, open('rf_gscv.pkl', 'wb'))\n",
    "\n",
    "rf_test_roc_auc = roc_auc_score(y_test, rf_y_test_pred_prob)\n",
    "rf_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 5524it [01:00, 77.22it/s]                           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Borrowing dependency',\n",
       " ' Interest Coverage Ratio (Interest expense to EBIT)',\n",
       " ' ROA(C) before interest and depreciation before interest',\n",
       " ' Continuous interest rate (after tax)',\n",
       " ' Inventory/Working Capital',\n",
       " ' Non-industry income and expenditure/revenue',\n",
       " ' Net Value Per Share (B)',\n",
       " ' Net Value Growth Rate',\n",
       " ' Per Share Net profit before tax (Yuan ¥)',\n",
       " ' Total debt/Total net worth',\n",
       " ' Net Income to Total Assets']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "for feat in numerical_features:\n",
    "    X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "    X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "selected_features = selected_features_dict['XGBClassifier'][1]['selected_feats']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_valid = X_valid[selected_features]\n",
    "\n",
    "model = selected_features_dict['XGBClassifier'][1]['model']\n",
    "explainer = shap.Explainer(model.predict, X_train, max_evals = int(2 * X_train.shape[1] + 1), verbose=0)\n",
    "shap_values = explainer(X_train)\n",
    "selected_shap_features = get_feature_importances_shap_values(\n",
    "    shap_values, features=list(X_train.columns), topk=10\n",
    ")\n",
    "selected_features_dict['XGBClassifier'][1]['selected_shap_feats'] = selected_shap_features\n",
    "selected_shap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 594 candidates, totalling 5940 fits\n",
      "{'eta': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.9371915753045379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9488980716253443"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_list = [0.01, 0.015, 0.025, 0.05, 0.1, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9]\n",
    "\n",
    "# gamma_list = [0, 0.05, 0.07, 0.09, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "max_depth_list = [3, 5, 6, 7, 9, 12, 15, 17, 25]\n",
    "\n",
    "# min_child_weight_list = [1, 3, 5, 7]\n",
    "\n",
    "# subsample_list = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# colsample_bytree_list = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# lambda_list = [0.01, 0.03, 0.1, 1.0]\n",
    "\n",
    "# alpha_list = [0, 0.1, 0.5, 1.0]\n",
    "\n",
    "n_estimators_list = [50, 100, 150, 200, 500, 1000]\n",
    "\n",
    "\n",
    "params_dict={\n",
    "    'eta': eta_list,\n",
    "    # 'gamma': gamma_list,\n",
    "    'max_depth': max_depth_list,\n",
    "    # 'min_child_weight': min_child_weight_list,\n",
    "    # 'subsample': subsample_list,\n",
    "    # 'colsample_bytree': colsample_bytree_list,\n",
    "    # 'lambda': lambda_list,\n",
    "    # 'alpha': alpha_list\n",
    "    'n_estimators': n_estimators_list,\n",
    "    # 'device': ['cuda']\n",
    "}\n",
    "\n",
    "xgb_gscv = GridSearchCV(\n",
    "    XGBClassifier(),\n",
    "    param_grid=params_dict,\n",
    "    scoring='roc_auc',\n",
    "    cv=skfold_list,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "\n",
    "# model training\n",
    "xgb_gscv.fit(data[selected_shap_features], data['Bankrupt?'])\n",
    "\n",
    "print(xgb_gscv.best_params_)\n",
    "print(xgb_gscv.best_score_)\n",
    "\n",
    "X_test = test_data[selected_shap_features]\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "X_train = data[selected_shap_features]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "xgb_y_train_pred_prob = xgb_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = xgb_gscv.predict(X_test[selected_features])\n",
    "xgb_y_test_pred_prob = xgb_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pickle.dump(xgb_gscv, open('xgb_gscv.pkl', 'wb'))\n",
    "\n",
    "xgb_test_roc_auc = roc_auc_score(y_test, xgb_y_test_pred_prob)\n",
    "xgb_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 5524it [00:24, 145.21it/s]                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Debt ratio %',\n",
       " ' Net worth/Assets',\n",
       " ' ROA(B) before interest and depreciation after tax',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " ' Net Income to Total Assets',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Net profit before tax/Paid-in capital',\n",
       " ' Per Share Net profit before tax (Yuan ¥)',\n",
       " ' Borrowing dependency',\n",
       " ' Liability to Equity',\n",
       " \" Net Income to Stockholder's Equity\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "for feat in numerical_features:\n",
    "    X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "    X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "scaler = selected_features_dict['LogisticRegression'][1]['scaler']\n",
    "\n",
    "X_train2 = scaler.transform(X_train[numerical_features])\n",
    "X_train2 = pd.DataFrame(X_train2, columns=numerical_features)\n",
    "X_train = pd.concat([X_train2, X_train[categorical_features]], axis=1)\n",
    "\n",
    "X_valid2 = scaler.transform(X_valid[numerical_features])\n",
    "X_valid2 = pd.DataFrame(X_valid2, columns=numerical_features)\n",
    "X_valid = pd.concat([X_valid2, X_valid[categorical_features]], axis=1)\n",
    "\n",
    "selected_features = selected_features_dict['LogisticRegression'][1]['selected_feats']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_valid = X_valid[selected_features]\n",
    "\n",
    "model = selected_features_dict['LogisticRegression'][1]['model']\n",
    "explainer = shap.Explainer(model.predict, X_train, max_evals = int(2 * X_train.shape[1] + 1), verbose=0)\n",
    "shap_values = explainer(X_train)\n",
    "selected_shap_features = get_feature_importances_shap_values(\n",
    "    shap_values, features=list(X_train.columns), topk=10\n",
    ")\n",
    "selected_features_dict['LogisticRegression'][1]['selected_shap_feats'] = selected_shap_features\n",
    "selected_shap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "{'lr__C': 0.01, 'lr__penalty': 'l2'}\n",
      "0.9268905699998715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9464187327823691"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "num_trans = Pipeline([('scale', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preproc', preprocessor),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "params_dict = {'lr__penalty': ['l1','l2'], 'lr__C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "\n",
    "lr_gscv = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid=params_dict,\n",
    "    scoring='roc_auc',\n",
    "    cv=skfold_list,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "\n",
    "# model training\n",
    "lr_gscv.fit(data[selected_shap_features], data['Bankrupt?'])\n",
    "\n",
    "print(lr_gscv.best_params_)\n",
    "print(lr_gscv.best_score_)\n",
    "\n",
    "X_test = test_data[selected_shap_features]\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "X_train = data[selected_shap_features]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "lr_y_train_pred_prob = lr_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = lr_gscv.predict(X_test[selected_features])\n",
    "lr_y_test_pred_prob = lr_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pickle.dump(lr_gscv, open('lr_gscv.pkl', 'wb'))\n",
    "\n",
    "lr_test_roc_auc = roc_auc_score(y_test, lr_y_test_pred_prob)\n",
    "lr_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ExactExplainer explainer: 5524it [1:23:58,  1.09it/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Liability to Equity',\n",
       " ' Borrowing dependency',\n",
       " ' Debt ratio %',\n",
       " ' Net worth/Assets',\n",
       " ' ROA(A) before interest and % after tax',\n",
       " ' Per Share Net profit before tax (Yuan ¥)',\n",
       " ' Persistent EPS in the Last Four Seasons',\n",
       " ' Net profit before tax/Paid-in capital',\n",
       " \" Net Income to Stockholder's Equity\",\n",
       " ' Liability-Assets Flag']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "for feat in numerical_features:\n",
    "    X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "    X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "scaler = selected_features_dict['SVC'][1]['scaler']\n",
    "\n",
    "X_train2 = scaler.transform(X_train[numerical_features])\n",
    "X_train2 = pd.DataFrame(X_train2, columns=numerical_features)\n",
    "X_train = pd.concat([X_train2, X_train[categorical_features]], axis=1)\n",
    "\n",
    "X_valid2 = scaler.transform(X_valid[numerical_features])\n",
    "X_valid2 = pd.DataFrame(X_valid2, columns=numerical_features)\n",
    "X_valid = pd.concat([X_valid2, X_valid[categorical_features]], axis=1)\n",
    "\n",
    "selected_features = selected_features_dict['SVC'][1]['selected_feats']\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_valid = X_valid[selected_features]\n",
    "\n",
    "model = selected_features_dict['SVC'][1]['model']\n",
    "explainer = shap.Explainer(model.predict, X_train)#max_evals = int(2 * X_train.shape[1] + 1), verbose=0\n",
    "shap_values = explainer(X_train)\n",
    "selected_shap_features = get_feature_importances_shap_values(\n",
    "    shap_values, features=list(X_train.columns), topk=10\n",
    ")\n",
    "selected_features_dict['SVC'][1]['selected_shap_feats'] = selected_shap_features\n",
    "selected_shap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "{'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.8998720616418318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9287878787878787"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "num_trans = Pipeline([('scale', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preproc', preprocessor),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "params_dict = {'svc__C': [1,10,100,1000], 'svc__gamma': [1,0.1,0.001,0.0001], 'svc__kernel': ['linear','rbf']}\n",
    "\n",
    "svc_gscv = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid=params_dict,\n",
    "    scoring='roc_auc',\n",
    "    cv=skfold_list,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "\n",
    "# model training\n",
    "svc_gscv.fit(data[selected_shap_features], data['Bankrupt?'])\n",
    "\n",
    "print(svc_gscv.best_params_)\n",
    "print(svc_gscv.best_score_)\n",
    "\n",
    "X_test = test_data[selected_shap_features]\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "X_train = data[selected_shap_features]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "svc_y_train_pred_prob = svc_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = svc_gscv.predict(X_test[selected_features])\n",
    "svc_y_test_pred_prob = svc_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "pickle.dump(svc_gscv, open('svc_gscv.pkl', 'wb'))\n",
    "\n",
    "svc_test_roc_auc = roc_auc_score(y_test, svc_y_test_pred_prob)\n",
    "svc_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving selected_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(selected_features_dict, open('selected_features_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the models and the selected features to avoid running the all cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gscv = pickle.load(open('rf_gscv.pkl', 'rb'))\n",
    "xgb_gscv = pickle.load(open('xgb_gscv.pkl', 'rb'))\n",
    "lr_gscv = pickle.load(open('lr_gscv.pkl', 'rb'))\n",
    "svc_gscv = pickle.load(open('svc_gscv.pkl', 'rb'))\n",
    "selected_features_dict = pickle.load(open('selected_features_dict.pkl', 'rb'))\n",
    "\n",
    "for feat in numerical_features:\n",
    "    test_data[f\"feat{numerical_features.index(feat)}\"] = test_data[feat] * test_data[' Liability-Assets Flag']\n",
    "\n",
    "# RandomForest calculations\n",
    "X_test = test_data[selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']]\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "X_train = data[selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "rf_y_train_pred_prob = rf_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = rf_gscv.predict(X_test[selected_features])\n",
    "rf_y_test_pred_prob = rf_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# XGBoost calculations\n",
    "X_test = test_data[selected_features_dict['XGBClassifier'][1]['selected_shap_feats']]\n",
    "\n",
    "X_train = data[selected_features_dict['XGBClassifier'][1]['selected_shap_feats']]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "xgb_y_train_pred_prob = xgb_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = xgb_gscv.predict(X_test[selected_features])\n",
    "xgb_y_test_pred_prob = xgb_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# LogisticRegression calculations\n",
    "X_test = test_data[selected_features_dict['LogisticRegression'][1]['selected_shap_feats']]\n",
    "\n",
    "X_train = data[selected_features_dict['LogisticRegression'][1]['selected_shap_feats']]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "lr_y_train_pred_prob = lr_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = lr_gscv.predict(X_test[selected_features])\n",
    "lr_y_test_pred_prob = lr_gscv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# SVC calculations\n",
    "X_test = test_data[selected_features_dict['SVC'][1]['selected_shap_feats']]\n",
    "\n",
    "X_train = data[selected_features_dict['SVC'][1]['selected_shap_feats']]\n",
    "# y_train = data['Bankrupt?'].to_frame()\n",
    "svc_y_train_pred_prob = svc_gscv.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# y_test_pred = svc_gscv.predict(X_test[selected_features])\n",
    "svc_y_test_pred_prob = svc_gscv.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9542011019283747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mean_ensemble_prediction(prob_list):\n",
    "    prob_array = np.vstack(prob_list).T\n",
    "    return np.mean(prob_array, axis=1)\n",
    "\n",
    "prob_list = [rf_y_test_pred_prob, xgb_y_test_pred_prob, lr_y_test_pred_prob, svc_y_test_pred_prob]\n",
    "avg_ens_y_test_pred_prob = get_mean_ensemble_prediction(prob_list)\n",
    "avg_ens_test_roc_auc = roc_auc_score(y_test, avg_ens_y_test_pred_prob)\n",
    "avg_ens_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553719008264463"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# ranked = []\n",
    "# for i in range(len(prob_list)):\n",
    "#     rank_data = stats.rankdata(prob_list[i])\n",
    "#     ranked.append(rank_data)\n",
    "\n",
    "# ranked = np.column_stack(ranked)\n",
    "\n",
    "# np.mean(ranked, axis=1)\n",
    "\n",
    "# Repespective AUC of test set: 0.9544765840220386, 0.9488980716253443, 0.9464187327823691, 0.9287878787878787\n",
    "\n",
    "rank_ens_y_test_pred_prob = (4*rf_y_test_pred_prob + 3*xgb_y_test_pred_prob + 2*lr_y_test_pred_prob + svc_y_test_pred_prob) / (1+2+3+4)\n",
    "rank_ens_test_roc_auc = roc_auc_score(y_test, rank_ens_y_test_pred_prob)\n",
    "rank_ens_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "class OptimizeAUC:\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _auc(self, coef, X, y):\n",
    "        X_coef = X * coef\n",
    "        preds = np.sum(X_coef, axis=1)\n",
    "        auc_score = roc_auc_score(y, preds)\n",
    "        return -1*auc_score\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._auc, X=X, y=y)\n",
    "        initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)\n",
    "        self.coef_ = fmin(loss_partial, initial_coef, disp=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_coef = X * self.coef_\n",
    "        preds = np.sum(X_coef, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.932744\n",
      "         Iterations: 10\n",
      "         Function evaluations: 59\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.946128\n",
      "         Iterations: 10\n",
      "         Function evaluations: 55\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.954882\n",
      "         Iterations: 43\n",
      "         Function evaluations: 103\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.934764\n",
      "         Iterations: 42\n",
      "         Function evaluations: 107\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.921296\n",
      "         Iterations: 34\n",
      "         Function evaluations: 93\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.932155\n",
      "         Iterations: 24\n",
      "         Function evaluations: 72\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.937626\n",
      "         Iterations: 14\n",
      "         Function evaluations: 60\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.959419\n",
      "         Iterations: 12\n",
      "         Function evaluations: 59\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.941609\n",
      "         Iterations: 31\n",
      "         Function evaluations: 90\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.944688\n",
      "         Iterations: 17\n",
      "         Function evaluations: 64\n"
     ]
    }
   ],
   "source": [
    "coef_dict = {}\n",
    "\n",
    "test_preds_list = []\n",
    "\n",
    "for feat in numerical_features:\n",
    "    test_data[f\"feat{numerical_features.index(feat)}\"] = test_data[feat] * test_data[' Liability-Assets Flag']\n",
    "\n",
    "X_test_rf = test_data[selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']]\n",
    "X_test_xgb = test_data[selected_features_dict['XGBClassifier'][1]['selected_shap_feats']]\n",
    "X_test_lr = test_data[selected_features_dict['LogisticRegression'][1]['selected_shap_feats']]\n",
    "X_test_svc = test_data[selected_features_dict['SVC'][1]['selected_shap_feats']]\n",
    "\n",
    "for idx in range(10):\n",
    "\n",
    "    X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    for feat in numerical_features:\n",
    "        X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "        X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "    # RandomForest\n",
    "    rf_selected_features = selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']\n",
    "    X_train_rf = X_train[rf_selected_features]\n",
    "    X_valid_rf = X_valid[rf_selected_features]\n",
    "\n",
    "    rfm = RandomForestClassifier(**rf_gscv.best_params_)\n",
    "    rfm.fit(X_train_rf, y_train)\n",
    "    rfm_valid_probs = rfm.predict_proba(X_valid_rf)[:, 1]\n",
    "\n",
    "    rfm_test_probs = rfm.predict_proba(X_test_rf)[:, 1]\n",
    "    # if idx == 0:\n",
    "    #     test_preds_dict['RandomForestClassifier'] = [rfm_test_probs]\n",
    "    # else:\n",
    "    #     test_preds_dict['RandomForestClassifier'].append(rfm_test_probs)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_selected_features = selected_features_dict['XGBClassifier'][1]['selected_shap_feats']\n",
    "    X_train_xgb = X_train[xgb_selected_features]\n",
    "    X_valid_xgb = X_valid[xgb_selected_features]\n",
    "\n",
    "    xgbm = XGBClassifier(**xgb_gscv.best_params_)\n",
    "    xgbm.fit(X_train_xgb, y_train)\n",
    "    xgbm_valid_probs = xgbm.predict_proba(X_valid_xgb)[:, 1]\n",
    "    xgbm_test_probs = xgbm.predict_proba(X_test_xgb)[:, 1]\n",
    "\n",
    "    # LogisticRegression\n",
    "    lr_selected_features = selected_features_dict['LogisticRegression'][1]['selected_shap_feats']\n",
    "    X_train_lr = X_train[lr_selected_features]\n",
    "    X_valid_lr = X_valid[lr_selected_features]\n",
    "\n",
    "    lr_params = {k.replace('lr__', ''): v for k, v in lr_gscv.best_params_.items()}\n",
    "    selected_shap_features = selected_features_dict['LogisticRegression'][1]['selected_shap_feats']\n",
    "    num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "    num_trans = Pipeline([('scale', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "    lrm = Pipeline(\n",
    "        [\n",
    "            ('preproc', preprocessor),\n",
    "            ('lr', LogisticRegression(**lr_params))\n",
    "        ]\n",
    "    )\n",
    "    lrm.fit(X_train_lr, y_train)\n",
    "    lrm_valid_probs = lrm.predict_proba(X_valid_lr)[:, 1]\n",
    "    lrm_test_probs = lrm.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "    # SVC\n",
    "    svc_selected_features = selected_features_dict['SVC'][1]['selected_shap_feats']\n",
    "    X_train_svc = X_train[svc_selected_features]\n",
    "    X_valid_svc = X_valid[svc_selected_features]\n",
    "\n",
    "    svc_params = {k.replace('svc__', ''): v for k, v in svc_gscv.best_params_.items()}\n",
    "    selected_shap_features = selected_features_dict['SVC'][1]['selected_shap_feats']\n",
    "    num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "    num_trans = Pipeline([('scale', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "    svcm = Pipeline(\n",
    "        [\n",
    "            ('preproc', preprocessor),\n",
    "            ('svc', SVC(probability=True, **svc_params))\n",
    "        ]\n",
    "    )\n",
    "    svcm.fit(X_train_svc, y_train)\n",
    "    svcm_valid_probs = svcm.predict_proba(X_valid_svc)[:, 1]\n",
    "    svcm_test_probs = svcm.predict_proba(X_test_svc)[:, 1]\n",
    "\n",
    "    fold_preds = np.column_stack([\n",
    "        rfm_valid_probs,\n",
    "        xgbm_valid_probs,\n",
    "        lrm_valid_probs,\n",
    "        svcm_valid_probs\n",
    "    ])\n",
    "\n",
    "    opt = OptimizeAUC()\n",
    "    opt.fit(fold_preds, y_valid)\n",
    "    coef_dict[idx] = opt.coef_\n",
    "\n",
    "    test_preds = np.column_stack([\n",
    "        rfm_test_probs,\n",
    "        xgbm_test_probs,\n",
    "        lrm_test_probs,\n",
    "        svcm_test_probs\n",
    "    ])\n",
    "\n",
    "    test_preds_list.append(opt.predict(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9567493112947658"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_y_test_pred_prob = np.mean(np.column_stack(test_preds_list), axis=1)\n",
    "opt_test_roc_auc = roc_auc_score(y_test, opt_y_test_pred_prob)\n",
    "opt_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(hidden_layer_sizes=(1, ))\n",
    "\n",
    "test_meta_feats_array = np.vstack([rf_y_test_pred_prob, xgb_y_test_pred_prob, lr_y_test_pred_prob, svc_y_test_pred_prob]).T\n",
    "train_meta_feats_array = np.vstack([rf_y_train_pred_prob, xgb_y_train_pred_prob, lr_y_train_pred_prob, svc_y_train_pred_prob]).T\n",
    "# train_meta_feats_array = np.vstack([rf_y_test_pred_prob, lr_y_test_pred_prob, svc_y_test_pred_prob]).T\n",
    "\n",
    "y_train = data['Bankrupt?'].to_frame()\n",
    "# y_train = test_data['Bankrupt?'].to_frame()\n",
    "mlp_model.fit(train_meta_feats_array, y_train)\n",
    "\n",
    "y_test = test_data['Bankrupt?'].to_frame()\n",
    "\n",
    "# test_meta_feats_array = np.vstack([rf_y_test_pred_prob, lr_y_test_pred_prob, svc_y_test_pred_prob]).T\n",
    "# test_meta_feats_array.shape\n",
    "\n",
    "mlp_y_test_pred_prob = mlp_model.predict_proba(test_meta_feats_array)[:, 1]\n",
    "mlp_test_roc_auc = roc_auc_score(y_test, mlp_y_test_pred_prob)\n",
    "mlp_test_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_dict = {}\n",
    "\n",
    "for feat in numerical_features:\n",
    "    test_data[f\"feat{numerical_features.index(feat)}\"] = test_data[feat] * test_data[' Liability-Assets Flag']\n",
    "\n",
    "X_test_rf = test_data[selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']]\n",
    "X_test_xgb = test_data[selected_features_dict['XGBClassifier'][1]['selected_shap_feats']]\n",
    "X_test_lr = test_data[selected_features_dict['LogisticRegression'][1]['selected_shap_feats']]\n",
    "X_test_svc = test_data[selected_features_dict['SVC'][1]['selected_shap_feats']]\n",
    "\n",
    "X_stack = np.zeros((data.shape[0], 4))\n",
    "\n",
    "for idx in range(10):\n",
    "\n",
    "    X_train = data.iloc[skfold_list[idx][0]].reset_index(drop=True)\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid = data.iloc[skfold_list[idx][1]].reset_index(drop=True)\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    for feat in numerical_features:\n",
    "        X_train[f\"feat{numerical_features.index(feat)}\"] = X_train[feat] * X_train[' Liability-Assets Flag']\n",
    "        X_valid[f\"feat{numerical_features.index(feat)}\"] = X_valid[feat] * X_valid[' Liability-Assets Flag']\n",
    "\n",
    "    # RandomForest\n",
    "    rf_selected_features = selected_features_dict['RandomForestClassifier'][1]['selected_shap_feats']\n",
    "    X_train_rf = X_train[rf_selected_features]\n",
    "    X_valid_rf = X_valid[rf_selected_features]\n",
    "\n",
    "    rfm = RandomForestClassifier(**rf_gscv.best_params_)\n",
    "    rfm.fit(X_train_rf, y_train)\n",
    "    rfm_valid_probs = rfm.predict_proba(X_valid_rf)[:, 1]\n",
    "    rfm_test_probs = rfm.predict_proba(X_test_rf)[:, 1]\n",
    "    X_stack[skfold_list[idx][1], 0] = rfm_valid_probs\n",
    "    # if idx == 0:\n",
    "    #     test_preds_dict['RandomForestClassifier'] = [rfm_test_probs]\n",
    "    # else:\n",
    "    #     test_preds_dict['RandomForestClassifier'].append(rfm_test_probs)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_selected_features = selected_features_dict['XGBClassifier'][1]['selected_shap_feats']\n",
    "    X_train_xgb = X_train[xgb_selected_features]\n",
    "    X_valid_xgb = X_valid[xgb_selected_features]\n",
    "\n",
    "    xgbm = XGBClassifier(**xgb_gscv.best_params_)\n",
    "    xgbm.fit(X_train_xgb, y_train)\n",
    "    xgbm_valid_probs = xgbm.predict_proba(X_valid_xgb)[:, 1]\n",
    "    xgbm_test_probs = xgbm.predict_proba(X_test_xgb)[:, 1]\n",
    "    X_stack[skfold_list[idx][1], 1] = xgbm_valid_probs\n",
    "\n",
    "    # LogisticRegression\n",
    "    lr_selected_features = selected_features_dict['LogisticRegression'][1]['selected_shap_feats']\n",
    "    X_train_lr = X_train[lr_selected_features]\n",
    "    X_valid_lr = X_valid[lr_selected_features]\n",
    "\n",
    "    lr_params = {k.replace('lr__', ''): v for k, v in lr_gscv.best_params_.items()}\n",
    "    selected_shap_features = selected_features_dict['LogisticRegression'][1]['selected_shap_feats']\n",
    "    num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "    num_trans = Pipeline([('scale', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "    lrm = Pipeline(\n",
    "        [\n",
    "            ('preproc', preprocessor),\n",
    "            ('lr', LogisticRegression(**lr_params))\n",
    "        ]\n",
    "    )\n",
    "    lrm.fit(X_train_lr, y_train)\n",
    "    lrm_valid_probs = lrm.predict_proba(X_valid_lr)[:, 1]\n",
    "    lrm_test_probs = lrm.predict_proba(X_test_lr)[:, 1]\n",
    "    X_stack[skfold_list[idx][1], 2] = lrm_valid_probs\n",
    "\n",
    "    # SVC\n",
    "    svc_selected_features = selected_features_dict['SVC'][1]['selected_shap_feats']\n",
    "    X_train_svc = X_train[svc_selected_features]\n",
    "    X_valid_svc = X_valid[svc_selected_features]\n",
    "\n",
    "    svc_params = {k.replace('svc__', ''): v for k, v in svc_gscv.best_params_.items()}\n",
    "    selected_shap_features = selected_features_dict['SVC'][1]['selected_shap_feats']\n",
    "    num_feat = [col for col in selected_shap_features if col in numerical_features]\n",
    "    num_trans = Pipeline([('scale', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers = [('num', num_trans, num_feat)], remainder='passthrough')\n",
    "    svcm = Pipeline(\n",
    "        [\n",
    "            ('preproc', preprocessor),\n",
    "            ('svc', SVC(probability=True, **svc_params))\n",
    "        ]\n",
    "    )\n",
    "    svcm.fit(X_train_svc, y_train)\n",
    "    svcm_valid_probs = svcm.predict_proba(X_valid_svc)[:, 1]\n",
    "    svcm_test_probs = svcm.predict_proba(X_test_svc)[:, 1]\n",
    "    X_stack[skfold_list[idx][1], 3] = svcm_valid_probs\n",
    "\n",
    "    test_preds = np.column_stack([\n",
    "        rfm_test_probs,\n",
    "        xgbm_test_probs,\n",
    "        lrm_test_probs,\n",
    "        svcm_test_probs\n",
    "    ])\n",
    "\n",
    "    test_preds_dict[idx] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6741735537190082"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_test_probs_list = []\n",
    "\n",
    "for idx in range(10):\n",
    "    temp_svc = SVC(probability=True)\n",
    "\n",
    "    X_train_stack = X_stack[skfold_list[idx][0], :]\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid_stack = X_stack[skfold_list[idx][1], :]\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    temp_svc.fit(X_train_stack, y_train)\n",
    "    temp_test_probs = temp_svc.predict_proba(test_preds_dict[idx])[:, 1]\n",
    "    svc_test_probs_list.append(temp_test_probs)\n",
    "\n",
    "stack_y_test_pred_prob = np.mean(np.column_stack(svc_test_probs_list), axis=1)\n",
    "stack_test_roc_auc = roc_auc_score(y_test, stack_y_test_pred_prob)\n",
    "stack_test_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462809917355371"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_test_probs_list = []\n",
    "\n",
    "for idx in range(10):\n",
    "    temp_lr = XGBClassifier()\n",
    "\n",
    "    X_train_stack = X_stack[skfold_list[idx][0], :]\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid_stack = X_stack[skfold_list[idx][1], :]\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    temp_lr.fit(X_train_stack, y_train)\n",
    "    temp_test_probs = temp_lr.predict_proba(test_preds_dict[idx])[:, 1]\n",
    "    lr_test_probs_list.append(temp_test_probs)\n",
    "\n",
    "stack_y_test_pred_prob = np.mean(np.column_stack(lr_test_probs_list), axis=1)\n",
    "stack_test_roc_auc = roc_auc_score(y_test, stack_y_test_pred_prob)\n",
    "stack_test_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9526859504132231"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test_probs_list = []\n",
    "\n",
    "for idx in range(10):\n",
    "    temp_rf = RandomForestClassifier()\n",
    "\n",
    "    X_train_stack = X_stack[skfold_list[idx][0], :]\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid_stack = X_stack[skfold_list[idx][1], :]\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    temp_rf.fit(X_train_stack, y_train)\n",
    "    temp_test_probs = temp_rf.predict_proba(test_preds_dict[idx])[:, 1]\n",
    "    rf_test_probs_list.append(temp_test_probs)\n",
    "\n",
    "stack_y_test_pred_prob = np.mean(np.column_stack(rf_test_probs_list), axis=1)\n",
    "stack_test_roc_auc = roc_auc_score(y_test, stack_y_test_pred_prob)\n",
    "stack_test_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462809917355371"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_test_probs_list = []\n",
    "\n",
    "for idx in range(10):\n",
    "    temp_xgb = XGBClassifier()\n",
    "\n",
    "    X_train_stack = X_stack[skfold_list[idx][0], :]\n",
    "    y_train = data.iloc[skfold_list[idx][0]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    X_valid_stack = X_stack[skfold_list[idx][1], :]\n",
    "    y_valid = data.iloc[skfold_list[idx][1]]['Bankrupt?'].to_frame().reset_index(drop=True)\n",
    "\n",
    "    temp_xgb.fit(X_train_stack, y_train)\n",
    "    temp_test_probs = temp_xgb.predict_proba(test_preds_dict[idx])[:, 1]\n",
    "    xgb_test_probs_list.append(temp_test_probs)\n",
    "\n",
    "stack_y_test_pred_prob = np.mean(np.column_stack(xgb_test_probs_list), axis=1)\n",
    "stack_test_roc_auc = roc_auc_score(y_test, stack_y_test_pred_prob)\n",
    "stack_test_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_main_path = '/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test'\n",
    "\n",
    "# # base data\n",
    "# test_df = pd.read_csv(f'{test_main_path}/test_base.csv')\n",
    "# test_df['date_decision'] = pd.to_datetime(test_df['date_decision'])\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# # static data\n",
    "# test_static_df = pd.DataFrame()\n",
    "# for file in ['test_static_0_0.csv', 'test_static_0_1.csv', 'test_static_0_2.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_static_df = pd.concat([test_static_df, temp_df], axis=0)\n",
    "\n",
    "# test_static_df.drop(columns=static_remove_cols_list, inplace=True)\n",
    "\n",
    "# print(f\"test_static_df shape: {test_static_df.shape}\")\n",
    "# test_df = pd.merge(test_df, test_static_df, on='case_id', how='left')\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in static_D_feats_list:\n",
    "#     test_df[col] = pd.to_datetime(test_df[col])\n",
    "#     test_df[f\"{col}_tdiff\"] = (test_df['date_decision'] - test_df[col]).dt.days\n",
    "#     test_df.drop(columns=col, inplace=True)\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_static_df\n",
    "\n",
    "# # external\n",
    "# test_static_cb_df = pd.read_csv(f'{test_main_path}/test_static_cb_0.csv')\n",
    "# test_static_cb_df.drop(columns=external_remove_cols_list, inplace=True)\n",
    "\n",
    "# print(f\"test_static_cb_df shape: {test_static_cb_df.shape}\")\n",
    "# test_df = pd.merge(test_df, test_static_cb_df, on='case_id', how='left')\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in external_D_feats_list:\n",
    "#     test_df[col] = pd.to_datetime(test_df[col])\n",
    "#     test_df[f\"{col}_tdiff\"] = (test_df['date_decision'] - test_df[col]).dt.days\n",
    "#     test_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_static_cb_df\n",
    "\n",
    "# # applprev depth=1\n",
    "# test_ddate_map = test_df.set_index('case_id')['date_decision'].to_dict()\n",
    "\n",
    "# test_applprev_df = pd.DataFrame()\n",
    "# for file in ['test_applprev_1_0.csv', 'test_applprev_1_1.csv', 'test_applprev_1_2.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_applprev_df = pd.concat([test_applprev_df, temp_df], axis=0)\n",
    "# test_applprev_df['date_decision'] = test_applprev_df['case_id'].map(test_ddate_map)\n",
    "\n",
    "# test_applprev_df.drop(columns=applprev1_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in applprev_D_feats_list:\n",
    "#     test_applprev_df[col] = pd.to_datetime(test_applprev_df[col])\n",
    "#     test_applprev_df[f\"{col}_tdiff\"] = (test_applprev_df['date_decision'] - test_applprev_df[col]).dt.days\n",
    "#     test_applprev_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_applprev_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_applprev_df[col].dtype == 'object':\n",
    "#         test_applprev_df[col] = test_applprev_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = applprev1_obj_map_dict[col]\n",
    "#         test_applprev_df[col] = test_applprev_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_applprev_df[test_applprev_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_applprev_df[test_applprev_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_applprev_df[test_applprev_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_applprev_df\n",
    "# gc.collect()\n",
    "\n",
    "# # other depth=1\n",
    "# test_other_df = pd.DataFrame()\n",
    "# for file in ['test_other_1.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_other_df = pd.concat([test_other_df, temp_df], axis=0)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_other_df.columns \n",
    "#     if col not in ['case_id', 'num_group1']\n",
    "# ]\n",
    "\n",
    "# test_other_df.drop(columns=other_remove_cols_list, inplace=True)\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_other_df[col].dtype == 'object':\n",
    "#         test_other_df[col] = test_other_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict =  other_obj_map_dict[col]\n",
    "#         test_other_df[col] = test_other_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_other_df[test_other_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_other_df[test_other_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_other_df[test_other_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_other_df\n",
    "# gc.collect()\n",
    "\n",
    "# # tax registry A\n",
    "# test_tax_registry_a_df = pd.DataFrame()\n",
    "# for file in ['test_tax_registry_a_1.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_tax_registry_a_df = pd.concat([test_tax_registry_a_df, temp_df], axis=0)\n",
    "# test_tax_registry_a_df['date_decision'] = test_tax_registry_a_df['case_id'].map(test_ddate_map)\n",
    "# test_tax_registry_a_df.drop(columns=tax_registry_a_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in tax_registry_a_D_feats_list:\n",
    "#     test_tax_registry_a_df[col] = pd.to_datetime(test_tax_registry_a_df[col])\n",
    "#     test_tax_registry_a_df[f\"{col}_tdiff\"] = (test_tax_registry_a_df['date_decision'] - test_tax_registry_a_df[col]).dt.days\n",
    "#     test_tax_registry_a_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_tax_registry_a_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_tax_registry_a_df[col].dtype == 'object':\n",
    "#         test_tax_registry_a_df[col] = test_tax_registry_a_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = tax_registry_a_obj_map_dict[col]\n",
    "#         test_tax_registry_a_df[col] = test_tax_registry_a_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_tax_registry_a_df[test_tax_registry_a_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_tax_registry_a_df[test_tax_registry_a_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_tax_registry_a_df[test_tax_registry_a_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_tax_registry_a_df\n",
    "# gc.collect()\n",
    "\n",
    "# # tax registry B\n",
    "# test_tax_registry_b_df = pd.DataFrame()\n",
    "# for file in ['test_tax_registry_b_1.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_tax_registry_b_df = pd.concat([test_tax_registry_b_df, temp_df], axis=0)\n",
    "# test_tax_registry_b_df['date_decision'] = test_tax_registry_b_df['case_id'].map(test_ddate_map)\n",
    "# test_tax_registry_b_df.drop(columns=tax_registry_b_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in tax_registry_b_D_feats_list:\n",
    "#     test_tax_registry_b_df[col] = pd.to_datetime(test_tax_registry_b_df[col])\n",
    "#     test_tax_registry_b_df[f\"{col}_tdiff\"] = (test_tax_registry_b_df['date_decision'] - test_tax_registry_b_df[col]).dt.days\n",
    "#     test_tax_registry_b_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_tax_registry_b_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_tax_registry_b_df[col].dtype == 'object':\n",
    "#         test_tax_registry_b_df[col] = test_tax_registry_b_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = tax_registry_b_obj_map_dict[col]\n",
    "#         test_tax_registry_b_df[col] = test_tax_registry_b_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_tax_registry_b_df[test_tax_registry_b_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_tax_registry_b_df[test_tax_registry_b_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_tax_registry_b_df[test_tax_registry_b_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_tax_registry_b_df\n",
    "# gc.collect()\n",
    "\n",
    "# # tax registry C\n",
    "# test_tax_registry_c_df = pd.DataFrame()\n",
    "# for file in ['test_tax_registry_c_1.csv']:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_tax_registry_c_df = pd.concat([test_tax_registry_c_df, temp_df], axis=0)\n",
    "# test_tax_registry_c_df['date_decision'] = test_tax_registry_c_df['case_id'].map(test_ddate_map)\n",
    "# test_tax_registry_c_df.drop(columns=tax_registry_c_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in tax_registry_c_D_feats_list:\n",
    "#     test_tax_registry_c_df[col] = pd.to_datetime(test_tax_registry_c_df[col])\n",
    "#     test_tax_registry_c_df[f\"{col}_tdiff\"] = (test_tax_registry_c_df['date_decision'] - test_tax_registry_c_df[col]).dt.days\n",
    "#     test_tax_registry_c_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_tax_registry_c_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_tax_registry_c_df[col].dtype == 'object':\n",
    "#         test_tax_registry_c_df[col] = test_tax_registry_c_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = tax_registry_c_obj_map_dict[col]\n",
    "#         test_tax_registry_c_df[col] = test_tax_registry_c_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_tax_registry_c_df[test_tax_registry_c_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_tax_registry_c_df[test_tax_registry_c_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_tax_registry_c_df[test_tax_registry_c_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_tax_registry_c_df\n",
    "# gc.collect()\n",
    "\n",
    "# # credit bureau A\n",
    "# test_credit_bureau_a_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_credit_bureau_a_1_0.csv', 'test_credit_bureau_a_1_1.csv', \n",
    "#     'test_credit_bureau_a_1_2.csv', 'test_credit_bureau_a_1_3.csv',\n",
    "#     'test_credit_bureau_a_1_4.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_credit_bureau_a_df = pd.concat([test_credit_bureau_a_df, temp_df], axis=0)\n",
    "# test_credit_bureau_a_df['date_decision'] = test_credit_bureau_a_df['case_id'].map(test_ddate_map)\n",
    "# test_credit_bureau_a_df.drop(columns=credit_bureau_a_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in credit_bureau_a_D_feats_list:\n",
    "#     test_credit_bureau_a_df[col] = pd.to_datetime(test_credit_bureau_a_df[col])\n",
    "#     test_credit_bureau_a_df[f\"{col}_tdiff\"] = (test_credit_bureau_a_df['date_decision'] - test_credit_bureau_a_df[col]).dt.days\n",
    "#     test_credit_bureau_a_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_credit_bureau_a_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_credit_bureau_a_df[col].dtype == 'object':\n",
    "#         test_credit_bureau_a_df[col] = test_credit_bureau_a_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = credit_bureau_a_obj_map_dict[col]\n",
    "#         test_credit_bureau_a_df[col] = test_credit_bureau_a_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_credit_bureau_a_df[test_credit_bureau_a_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_credit_bureau_a_df[test_credit_bureau_a_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_credit_bureau_a_df[test_credit_bureau_a_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_credit_bureau_a_df\n",
    "# gc.collect()\n",
    "\n",
    "# # credit bureau B\n",
    "# test_credit_bureau_b_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_credit_bureau_b_1.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_credit_bureau_b_df = pd.concat([test_credit_bureau_b_df, temp_df], axis=0)\n",
    "# test_credit_bureau_b_df['date_decision'] = test_credit_bureau_b_df['case_id'].map(test_ddate_map)\n",
    "# test_credit_bureau_b_df.drop(columns=credit_bureau_b_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in credit_bureau_b_D_feats_list:\n",
    "#     test_credit_bureau_b_df[col] = pd.to_datetime(test_credit_bureau_b_df[col])\n",
    "#     test_credit_bureau_b_df[f\"{col}_tdiff\"] = (test_credit_bureau_b_df['date_decision'] - test_credit_bureau_b_df[col]).dt.days\n",
    "#     test_credit_bureau_b_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_credit_bureau_b_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_credit_bureau_b_df[col].dtype == 'object':\n",
    "#         test_credit_bureau_b_df[col] = test_credit_bureau_b_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = credit_bureau_b_obj_map_dict[col]\n",
    "#         test_credit_bureau_b_df[col] = test_credit_bureau_b_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_credit_bureau_b_df[test_credit_bureau_b_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_credit_bureau_b_df[test_credit_bureau_b_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_credit_bureau_b_df[test_credit_bureau_b_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_credit_bureau_b_df\n",
    "# gc.collect()\n",
    "\n",
    "# # deposit depth=1\n",
    "# test_deposit_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_deposit_1.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_deposit_df = pd.concat([test_deposit_df, temp_df], axis=0)\n",
    "# test_deposit_df['date_decision'] = test_deposit_df['case_id'].map(test_ddate_map)\n",
    "# test_deposit_df.drop(columns=deposit_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in deposit_D_feats_list:\n",
    "#     test_deposit_df[col] = pd.to_datetime(test_deposit_df[col])\n",
    "#     test_deposit_df[f\"{col}_tdiff\"] = (test_deposit_df['date_decision'] - test_deposit_df[col]).dt.days\n",
    "#     test_deposit_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_deposit_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_deposit_df[col].dtype == 'object':\n",
    "#         test_deposit_df[col] = test_deposit_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = deposit_obj_map_dict[col]\n",
    "#         test_deposit_df[col] = test_deposit_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_deposit_df[test_deposit_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_deposit_df[test_deposit_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_deposit_df[test_deposit_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_deposit_df\n",
    "# gc.collect()\n",
    "\n",
    "# # person depth=1\n",
    "# test_person_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_person_1.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_person_df = pd.concat([test_person_df, temp_df], axis=0)\n",
    "# test_person_df['date_decision'] = test_person_df['case_id'].map(test_ddate_map)\n",
    "# test_person_df.drop(columns=person_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in person_D_feats_list:\n",
    "#     test_person_df[col] = pd.to_datetime(test_person_df[col])\n",
    "#     test_person_df[f\"{col}_tdiff\"] = (test_person_df['date_decision'] - test_person_df[col]).dt.days\n",
    "#     test_person_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_person_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_person_df[col].dtype == 'object':\n",
    "#         test_person_df[col] = test_person_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = person_obj_map_dict[col]\n",
    "#         test_person_df[col] = test_person_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_person_df[test_person_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_person_df[test_person_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_person_df[test_person_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_person_df\n",
    "# gc.collect()\n",
    "\n",
    "# # debitcard depth=1\n",
    "# test_debitcard_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_debitcard_1.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_debitcard_df = pd.concat([test_debitcard_df, temp_df], axis=0)\n",
    "# test_debitcard_df['date_decision'] = test_debitcard_df['case_id'].map(test_ddate_map)\n",
    "# test_debitcard_df.drop(columns=debitcard_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in debitcard_D_feats_list:\n",
    "#     test_debitcard_df[col] = pd.to_datetime(test_debitcard_df[col])\n",
    "#     test_debitcard_df[f\"{col}_tdiff\"] = (test_debitcard_df['date_decision'] - test_debitcard_df[col]).dt.days\n",
    "#     test_debitcard_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_debitcard_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_debitcard_df[col].dtype == 'object':\n",
    "#         test_debitcard_df[col] = test_debitcard_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = debitcard_obj_map_dict[col]\n",
    "#         test_debitcard_df[col] = test_debitcard_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_debitcard_df[test_debitcard_df['num_group1']==0].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_debitcard_df[test_debitcard_df['num_group1']==0][col].median()\n",
    "#         temp_map_dict = test_debitcard_df[test_debitcard_df['num_group1']==0].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_debitcard_df\n",
    "# gc.collect()\n",
    "\n",
    "# # applprev depth=2\n",
    "# test_applprev2_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_applprev_2.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_applprev2_df = pd.concat([test_applprev2_df, temp_df], axis=0)\n",
    "# test_applprev2_df['date_decision'] = test_applprev2_df['case_id'].map(test_ddate_map)\n",
    "# test_applprev2_df.drop(columns=applprev_2_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in applprev_2_D_feats_list:\n",
    "#     test_applprev2_df[col] = pd.to_datetime(test_applprev2_df[col])\n",
    "#     test_applprev2_df[f\"{col}_tdiff\"] = (test_applprev2_df['date_decision'] - test_applprev2_df[col]).dt.days\n",
    "#     test_applprev2_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_applprev2_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_applprev2_df[col].dtype == 'object':\n",
    "#         test_applprev2_df[col] = test_applprev2_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = applprev_2_obj_map_dict[col]\n",
    "#         test_applprev2_df[col] = test_applprev2_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_applprev2_df[(test_applprev2_df['num_group1']==0) & (test_applprev2_df['num_group2']==0)].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_applprev2_df[(test_applprev2_df['num_group1']==0) & (test_applprev2_df['num_group2']==0)][col].median()\n",
    "#         temp_map_dict = test_applprev2_df[(test_applprev2_df['num_group1']==0) & (test_applprev2_df['num_group2']==0)].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_applprev2_df\n",
    "# gc.collect()\n",
    "\n",
    "# # person depth=2\n",
    "# test_person2_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_person_2.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_person2_df = pd.concat([test_person2_df, temp_df], axis=0)\n",
    "# test_person2_df['date_decision'] = test_person2_df['case_id'].map(test_ddate_map)\n",
    "# test_person2_df.drop(columns=person2_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in person2_D_feats_list:\n",
    "#     test_person2_df[col] = pd.to_datetime(test_person2_df[col])\n",
    "#     test_person2_df[f\"{col}_tdiff\"] = (test_person2_df['date_decision'] - test_person2_df[col]).dt.days\n",
    "#     test_person2_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_person2_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_person2_df[col].dtype == 'object':\n",
    "#         test_person2_df[col] = test_person2_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = person2_obj_map_dict[col]\n",
    "#         test_person2_df[col] = test_person2_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_person2_df[(test_person2_df['num_group1']==0) & (test_person2_df['num_group2']==0)].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_person2_df[(test_person2_df['num_group1']==0) & (test_person2_df['num_group2']==0)][col].median()\n",
    "#         temp_map_dict = test_person2_df[(test_person2_df['num_group1']==0) & (test_person2_df['num_group2']==0)].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_person2_df\n",
    "# gc.collect()\n",
    "\n",
    "# # credit bureau A depth=2\n",
    "# test_credit_bureau_a2_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_credit_bureau_a_2_0.csv',\n",
    "#     'test_credit_bureau_a_2_1.csv',\n",
    "#     'test_credit_bureau_a_2_2.csv',\n",
    "#     'test_credit_bureau_a_2_3.csv',\n",
    "#     'test_credit_bureau_a_2_4.csv',\n",
    "#     'test_credit_bureau_a_2_5.csv',\n",
    "#     'test_credit_bureau_a_2_6.csv',\n",
    "#     'test_credit_bureau_a_2_7.csv',\n",
    "#     'test_credit_bureau_a_2_8.csv',\n",
    "#     'test_credit_bureau_a_2_9.csv',\n",
    "#     'test_credit_bureau_a_2_10.csv',\n",
    "#     'test_credit_bureau_a_2_11.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_credit_bureau_a2_df = pd.concat([test_credit_bureau_a2_df, temp_df], axis=0)\n",
    "# test_credit_bureau_a2_df['date_decision'] = test_credit_bureau_a2_df['case_id'].map(test_ddate_map)\n",
    "# test_credit_bureau_a2_df.drop(columns=credit_bureau_a2_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in credit_bureau_a2_D_feats_list:\n",
    "#     test_credit_bureau_a2_df[col] = pd.to_datetime(test_credit_bureau_a2_df[col])\n",
    "#     test_credit_bureau_a2_df[f\"{col}_tdiff\"] = (test_credit_bureau_a2_df['date_decision'] - test_credit_bureau_a2_df[col]).dt.days\n",
    "#     test_credit_bureau_a2_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_credit_bureau_a2_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_credit_bureau_a2_df[col].dtype == 'object':\n",
    "#         test_credit_bureau_a2_df[col] = test_credit_bureau_a2_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = credit_bureau_a2_obj_map_dict[col]\n",
    "#         test_credit_bureau_a2_df[col] = test_credit_bureau_a2_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_credit_bureau_a2_df[(test_credit_bureau_a2_df['num_group1']==0) & (test_credit_bureau_a2_df['num_group2']==0)].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_credit_bureau_a2_df[(test_credit_bureau_a2_df['num_group1']==0) & (test_credit_bureau_a2_df['num_group2']==0)][col].median()\n",
    "#         temp_map_dict = test_credit_bureau_a2_df[(test_credit_bureau_a2_df['num_group1']==0) & (test_credit_bureau_a2_df['num_group2']==0)].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_credit_bureau_a2_df\n",
    "# gc.collect()\n",
    "\n",
    "# # credit bureau B depth=2\n",
    "# test_credit_bureau_b2_df = pd.DataFrame()\n",
    "# for file in [\n",
    "#     'test_credit_bureau_b_2.csv'\n",
    "# ]:\n",
    "#     temp_df = pd.read_csv(f'{test_main_path}/{file}')\n",
    "#     test_credit_bureau_b2_df = pd.concat([test_credit_bureau_b2_df, temp_df], axis=0)\n",
    "# test_credit_bureau_b2_df['date_decision'] = test_credit_bureau_b2_df['case_id'].map(test_ddate_map)\n",
    "# test_credit_bureau_b2_df.drop(columns=credit_bureau_b2_remove_cols_list, inplace=True)\n",
    "\n",
    "# for col in credit_bureau_b2_D_feats_list:\n",
    "#     test_credit_bureau_b2_df[col] = pd.to_datetime(test_credit_bureau_b2_df[col])\n",
    "#     test_credit_bureau_b2_df[f\"{col}_tdiff\"] = (test_credit_bureau_b2_df['date_decision'] - test_credit_bureau_b2_df[col]).dt.days\n",
    "#     test_credit_bureau_b2_df.drop(columns=col, inplace=True)\n",
    "    \n",
    "# temp_feats_list = [\n",
    "#     col for col in test_credit_bureau_b2_df.columns \n",
    "#     if col not in ['case_id', 'num_group1', 'date_decision']\n",
    "# ]\n",
    "\n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# for col in tqdm(temp_feats_list):\n",
    "#     if test_credit_bureau_b2_df[col].dtype == 'object':\n",
    "#         test_credit_bureau_b2_df[col] = test_credit_bureau_b2_df[col].fillna('Unknown') #, inplace=True\n",
    "#         temp_map_dict = credit_bureau_b2_obj_map_dict[col]\n",
    "#         test_credit_bureau_b2_df[col] = test_credit_bureau_b2_df[col].apply(lambda x: temp_map_dict.get(x, len(temp_map_dict)))\n",
    "#         temp_map_dict2 = test_credit_bureau_b2_df[(test_credit_bureau_b2_df['num_group1']==0) & (test_credit_bureau_b2_df['num_group2']==0)].\\\n",
    "#             groupby('case_id')[col].agg(pd.Series.mode).to_dict()\n",
    "#         test_df[col] = test_df['case_id'].map(temp_map_dict2)\n",
    "#     else:\n",
    "#         temp_median = test_credit_bureau_b2_df[(test_credit_bureau_b2_df['num_group1']==0) & (test_credit_bureau_b2_df['num_group2']==0)][col].median()\n",
    "#         temp_map_dict = test_credit_bureau_b2_df[(test_credit_bureau_b2_df['num_group1']==0) & (test_credit_bureau_b2_df['num_group2']==0)].fillna(temp_median).\\\n",
    "#             groupby('case_id')[col].\\\n",
    "#             agg(['mean', 'median', 'std', 'max', 'min', 'skew']).to_dict()\n",
    "#         for k in temp_map_dict:\n",
    "#             test_df[f\"{col}_{k}\"] = test_df['case_id'].map(temp_map_dict[k])\n",
    "            \n",
    "# print(f\"test_df shape: {test_df.shape}\")\n",
    "\n",
    "# del test_credit_bureau_b2_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_dpd2bins(x):\n",
    "#     if x >= 0 and x <= 30:\n",
    "#         return 0\n",
    "#     elif x >= 31 and x <= 60:\n",
    "#         return 1\n",
    "#     elif x >= 61 and x <= 90:\n",
    "#         return 2\n",
    "#     elif x >= 91:\n",
    "#         return 3\n",
    "#     elif pd.isna(x):\n",
    "#         return 4\n",
    "\n",
    "# A_feats_list = [col for col in train_df.columns if col.endswith('A')]\n",
    "# P_feats_list = [col for col in train_df.columns if col.endswith('P')]\n",
    "# L_feats_list = [col for col in train_df.columns if col.endswith('L')]\n",
    "# D_feats_list = [col for col in train_df.columns if col.endswith('D')]\n",
    "# M_feats_list = [col for col in train_df.columns if col.endswith('M')]\n",
    "# T_feats_list = [col for col in train_df.columns if col.endswith('T')]\n",
    "\n",
    "# feats_map_dict = {}\n",
    "\n",
    "# exceptional_columns = ['date_decision', 'MONTH', 'WEEK_NUM', 'target']\n",
    "# feats_list = [col for col in train_df.columns if col not in exceptional_columns and col not in D_feats_list]\n",
    "\n",
    "# drop_cols_list = []\n",
    "\n",
    "# for col in feats_list:\n",
    "\n",
    "#     if train_df[col].isnull().sum() / train_df.shape[0] >= 0.9:\n",
    "#         drop_cols_list.append(col)\n",
    "#         continue\n",
    "\n",
    "#     if train_df[col].dtype == 'object':\n",
    "#         train_df[col].fillna('Unknown', inplace=True)\n",
    "#         temp_unique_list = list(train_df[col].unique())\n",
    "#         temp_map_dict = {temp_unique_list[i]: i for i in range(len(temp_unique_list))}\n",
    "#         train_df[col] = train_df[col].map(temp_map_dict)\n",
    "#         feats_map_dict[col] = temp_map_dict\n",
    "#     else:\n",
    "#         train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "#     if col.endswith('P'):\n",
    "#         train_df[f'{col}_bin'] = train_df[col].apply(convert_dpd2bins)\n",
    "\n",
    "# # for col in P_feats_list:\n",
    "# #     train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "# #     train_df[f'{col}_bin'] = train_df[col].apply(convert_dpd2bins)\n",
    "\n",
    "# # for col in L_feats_list:\n",
    "# #     train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "# datefirstoffer_1144D: Date of first customer relationship management (CRM) offer. dtype=object missing=560713 (55.86%)\n",
    "# datelastinstal40dpd_247D: Date of last instalment that was more than 40 days past due (DPD). dtype=object missing=932264 (92.88%)\n",
    "# datelastunpaid_3546854D: Date of the last unpaid instalment. dtype=object missing=606750 (60.45%)\n",
    "# dtlastpmtallstes_4499206D: Date of last payment made by the applicant. dtype=object missing=758904 (75.61%)\n",
    "# firstclxcampaign_1125D: Date of the client's first campaign. dtype=object missing=571655 (56.95%)\n",
    "# firstdatedue_489D: Date of the first due date. dtype=object missing=339687 (33.84%)\n",
    "# lastactivateddate_801D: Contract activation date for previous applications. dtype=object missing=320842 (31.96%)\n",
    "# lastapplicationdate_877D: Date of previous customer's application. dtype=object missing=220760 (21.99%)\n",
    "# lastapprdate_640D: Date of approval on client's most recent previous application. dtype=object missing=313123 (31.20%)\n",
    "# lastdelinqdate_224D: Date of the last delinquency occurrence. dtype=object missing=668680 (66.62%)\n",
    "# lastrejectdate_50D: Date of most recent rejected application by the applicant. dtype=object missing=530899 (52.89%)\n",
    "# lastrepayingdate_696D: Date of the last payment made by the applicant. dtype=object missing=1002152 (99.84%)\n",
    "# maxdpdinstldate_3546855D: Date of instalment on which client was most days past due. dtype=object missing=567808 (56.57%)\n",
    "# payvacationpostpone_4187118D: Date of last payment holiday instalment. dtype=object missing=1002290 (99.85%)\n",
    "# validfrom_1069D: Date since the client has an active campaign. dtype=object missing=884352 (88.10%)\n",
    "\n",
    "# assignmentdate_238D: Tax authority data - date of assignment. dtype=object missing=1363480 (90.87%)\n",
    "# assignmentdate_4527235D: Tax authority data - Date of assignment. dtype=object missing=1385498 (92.34%)\n",
    "# assignmentdate_4955616D: Tax authority assignment date. dtype=object missing=1428843 (95.23%)\n",
    "# birthdate_574D: Client's date of birth (credit bureau data). dtype=object missing=892605 (59.49%)\n",
    "# dateofbirth_337D: Client's date of birth. dtype=object missing=114785 (7.65%)\n",
    "# dateofbirth_342D: Client's date of birth. dtype=object missing=1463976 (97.57%)\n",
    "# responsedate_1012D: Tax authority's response date. dtype=object missing=780476 (52.02%)\n",
    "# responsedate_4527233D: Tax authority's response date. dtype=object missing=840149 (55.99%)\n",
    "# responsedate_4917613D: Tax authority's response date. dtype=object missing=1275564 (85.01%)\n",
    "\n",
    "# approvaldate_319D: Approval Date of Previous Application dtype=object missing=1766021 (45.43%)\n",
    "# creationdate_885D: Date when previous application was created. dtype=object missing=35 (0.00%)\n",
    "# dateactivated_425D: Contract activation date of the applicant's previous application. dtype=object missing=1844702 (47.45%)\n",
    "# dtlastpmt_581D: Date of last payment made by the applicant. dtype=object missing=2860375 (73.58%)\n",
    "# dtlastpmtallstes_3545839D: Date of the applicant's last payment. dtype=object missing=2434155 (62.61%)\n",
    "# employedfrom_700D: Employment start date from the previous application. dtype=object missing=2180869 (56.10%)\n",
    "# firstnonzeroinstldate_307D: Date of first instalment in the previous application. dtype=object missing=365175 (9.39%)\n",
    "\n",
    "# for3years_504L: Client's credit history data over the last three years. dtype=float64 missing=1463962 (97.57%)\n",
    "# formonth_535L: Credit history for the last month. dtype=float64 missing=1463962 (97.57%)\n",
    "# forquarter_634L: Credit history for the last quarter. dtype=float64 missing=1463962 (97.57%)\n",
    "# fortoday_1092L: Client's credit history for today. dtype=float64 missing=1463962 (97.57%)\n",
    "# forweek_528L: Credit history for the last week. dtype=float64 missing=1463962 (97.57%)\n",
    "# foryear_850L: Credit history for the last year. dtype=float64 missing=1463962 (97.57%)\n",
    "\n",
    "# recorddate_4527225D: Date of tax deduction record. dtype=object missing=0 (0.00%)\n",
    "\n",
    "# deductiondate_4917603D: Tax deduction date. dtype=object missing=0 (0.00%)\n",
    "\n",
    "# processingdate_168D: Date when the tax deduction is processed. dtype=object missing=0 (0.00%)\n",
    "\n",
    "# dateofcredend_289D: End date of an active credit contract. dtype=object missing=3449815 (83.97%)\n",
    "# dateofcredend_353D: End date of a closed credit contract. dtype=object missing=3651494 (88.88%)\n",
    "# dateofcredstart_181D: Date when the credit contract was closed. dtype=object missing=3651491 (88.88%)\n",
    "# dateofcredstart_739D: Start date of a closed credit contract. dtype=object missing=3449815 (83.97%)\n",
    "# dateofrealrepmt_138D: Date of credit's closure (contract termination date). dtype=object missing=3654366 (88.95%)\n",
    "# numberofoverdueinstlmaxdat_148D: Date of maximum number of past due instalments for the closed contract. dtype=object missing=1701590 (81.83%)\n",
    "# numberofoverdueinstlmaxdat_641D: Date of maximum number of past due instalments for the active contract. dtype=object missing=1990461 (95.73%)\n",
    "# overdueamountmax2date_1002D: Date of maximal past due amount for a closed contract dtype=object missing=1705299 (82.01%)\n",
    "# overdueamountmax2date_1142D: Date of maximal past due amount for an active contract. dtype=object missing=1989828 (95.70%)\n",
    "# refreshdate_3813885D: Date when the credit bureau's public sources have been last updated. dtype=object missing=666459 (32.05%)\n",
    "\n",
    "# contractdate_551D: Contract date of the active contract dtype=object missing=3892 (4.54%)\n",
    "# contractmaturitydate_151D: End date of active contract. dtype=object missing=4079 (4.75%)\n",
    "# lastupdate_260D: Last update date for the active contracts. dtype=object missing=3892 (4.54%)\n",
    "\n",
    "# contractenddate_991D: End date of deposit contract. dtype=object missing=79682 (54.92%)\n",
    "# openingdate_313D: Deposit account opening date. dtype=object missing=0 (0.00%)\n",
    "\n",
    "# birth_259D: Date of birth of the person. dtype=object missing=1447332 (48.67%)\n",
    "# birthdate_87D: Birth date of the person. dtype=object missing=2949075 (99.16%)\n",
    "# empl_employedfrom_271D: Start date of employment. dtype=object missing=2407290 (80.94%)\n",
    "\n",
    "# openingdate_857D: Debit card opening date. dtype=object missing=12711 (8.08%)\n",
    "\n",
    "# empls_employedfrom_796D: Start of employment (num_group1 - person, num_group2 - employment). dtype=object missing=1637653 (99.65%)\n",
    "\n",
    "# pmts_date_1107D: Payment date for an active contract according to credit bureau (num_group1 - contract, num_group2 - payment). dtype=object missing=0 (0.00%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
